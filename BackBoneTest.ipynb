{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56270576-6b91-44c0-b748-2669a3459d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from arcface.dataset import build_dataset\n",
    "from arcface.losses import ArcLoss\n",
    "from arcface.network import ArcLayer, L2Normalization, hrnet_v2\n",
    "from arcface.training_supervisor import TrainingSupervisor\n",
    "\n",
    "import IPython.display as display\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e51799-16e5-42fb-8357-a6b222a3dcf5",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "1. hrnet (Done)\n",
    "2. resnet ()\n",
    "3. efficienthrnet\n",
    "4. efficientnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec79e2-e72c-4b4f-a95c-062781b88a26",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c655c8ad-aca2-444a-b455-e1c04897b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_example(image, label):\n",
    "    feature = {\n",
    "        \"image\": image_feature(image),\n",
    "        \"label\": bytes_feature(label),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "### Backbone Class make (ToDo Make into a separate module)############################\n",
    "class RecordOperator(ABC):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        # Construct a reader if the user is trying to read the record file.\n",
    "        self.dataset = None\n",
    "        self._writer = None\n",
    "        if tf.io.gfile.exists(filename):\n",
    "            self.dataset = tf.data.TFRecordDataset(filename)\n",
    "        else:\n",
    "            # Construct a writer in case the user want to write something.\n",
    "            self._writer = tf.io.TFRecordWriter(filename)\n",
    "\n",
    "        # Set the feature description. This should be provided before trying to\n",
    "        # parse the record file.\n",
    "        self.set_feature_description()\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_example(self):\n",
    "        \"\"\"Returns a tf.train.example from values to be saved.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def write_example(self, tf_example):\n",
    "        \"\"\"Create TFRecord example from a data sample.\"\"\"\n",
    "        if self._writer is None:\n",
    "            raise IOError(\"Record file already exists.\")\n",
    "        else:\n",
    "            self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_feature_description(self):\n",
    "        \"\"\"Set the feature_description to parse TFRecord file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def parse_dataset(self):\n",
    "        # Create a dictionary describing the features. This dict should be\n",
    "        # consistent with the one used while generating the record file.\n",
    "        if self.dataset is None:\n",
    "            raise IOError(\"Dataset file not found.\")\n",
    "\n",
    "        def _parse_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            return tf.io.parse_single_example(example_proto, self.feature_description)\n",
    "\n",
    "        parsed_dataset = self.dataset.map(_parse_function)\n",
    "        return parsed_dataset\n",
    "#########################################################################################\n",
    "\n",
    "class ImageDataset(RecordOperator):\n",
    "    \"\"\"Construct ImageDataset tfrecord files.\"\"\"\n",
    "\n",
    "    def make_example(self, image, label):\n",
    "        \"\"\"Construct an tf.Example with image data and label.\n",
    "        Args:\n",
    "            image_string: encoded image, NOT as numpy array.\n",
    "            label: the label.\n",
    "        Returns:\n",
    "            a tf.Example.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_string = tf.image.decode_image(image)\n",
    "        image_shape = image_string.shape\n",
    "        \n",
    "\n",
    "        # After getting all the features, time to generate a TensorFlow example.\n",
    "        feature = {\n",
    "            'image/height': int64_feature(image_shape[0]),\n",
    "            'image/width': int64_feature(image_shape[1]),\n",
    "            'image/depth': int64_feature(image_shape[2]),\n",
    "            'image/encoded': image_feature(image_string),\n",
    "            'label': int64_feature(label),\n",
    "        }\n",
    "\n",
    "        tf_example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "\n",
    "        return tf_example\n",
    "    \n",
    "    def set_feature_description(self):\n",
    "        self.feature_description = {\n",
    "            \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/encoded\": tf.io.VarLenFeature(tf.float32),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "#         example = tf.io.parse_single_example(example, feature_description)\n",
    "#         example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "#         return example\n",
    "\n",
    "\n",
    "def create_tfrecord(path = 'datasets/sorted_palmvein_roi/*/*.bmp', tf_record='datasets/train.record'):\n",
    "    \n",
    "    converter = ImageDataset(tf_record)\n",
    "    samples = glob(path)\n",
    "    total_samples_num = len(samples)\n",
    "    ids = set()\n",
    "    print(\"Total records: {}\".format(total_samples_num))\n",
    "    \n",
    "    for i, image_path in tqdm(enumerate(samples)):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        ids.add(image_path.split('/')[-2])\n",
    "        label = len(ids)\n",
    "        tf_example = converter.make_example(image, label)\n",
    "        # Write the example to file.\n",
    "        converter.write_example(tf_example)\n",
    "        \n",
    "    print(\"All done. Record file is:\\n{}\".format(tf_record))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abbc9f2-9561-4be1-941a-858e4540f80f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2022-02-20 19:42:40.850230: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-20 19:42:40.873349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-02-20 19:42:41.392367: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:41.392493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 Ti computeCapability: 7.5\n",
      "coreClock: 1.485GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2022-02-20 19:42:41.392627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-20 19:42:41.718388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-20 19:42:41.718519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-20 19:42:41.886965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-20 19:42:41.966563: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-20 19:42:42.291458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-20 19:42:42.352532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-20 19:42:43.083384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-20 19:42:43.084184: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:43.084891: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:43.084907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-20 19:42:43.094268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-20 19:42:43.108518: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-20 19:42:43.109238: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:43.109266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 Ti computeCapability: 7.5\n",
      "coreClock: 1.485GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2022-02-20 19:42:43.109336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-20 19:42:43.109363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-20 19:42:43.109379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-20 19:42:43.109394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-20 19:42:43.109410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-20 19:42:43.109426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-20 19:42:43.109441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-20 19:42:43.109457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-20 19:42:43.110030: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:43.110533: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:43.110544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-20 19:42:43.123521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-20 19:42:49.089702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-02-20 19:42:49.089793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-02-20 19:42:49.089798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-02-20 19:42:49.118512: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:49.118538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-02-20 19:42:49.119323: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:49.120039: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-20 19:42:49.120076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2877 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "12000it [01:01, 195.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Record file is:\n",
      "datasets/train_processed.record\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord(path= 'datasets/processed/*/*.bmp',  tf_record='datasets/train_processed.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa326f-43af-4dbb-890a-531af22b71a1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdd0f44-d827-43b0-b3fe-321d94100ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_model, name = \"hrnetv2\", train_files = \"datasets/train.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 600, num_examples = 12000, training_dir = os.getcwd(),\n",
    "          frequency = 1000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 50, restore_weights_only=False):\n",
    "\n",
    "    '''\n",
    "    # Deep neural network training is complicated. The first thing is making\n",
    "    # sure you have everything ready for training, like datasets, checkpoints,\n",
    "    # logs, etc. Modify these paths to suit your needs.\n",
    "\n",
    "    name:str = # What is the model's name?\n",
    "    \n",
    "    train_files:str = # Where are the training files?\n",
    "\n",
    "    test_files:str = # Where are the testing files?\n",
    "\n",
    "    val_files:str = # Where are the validation files? Set `None` if no files available.\n",
    "\n",
    "    input_shape:tuple(int) = # What is the shape of the input image?\n",
    "\n",
    "    embedding_size:int = # What is the size of the embeddings that represent the faces?\n",
    "\n",
    "    num_ids:int = # How many identities do you have in the training dataset?\n",
    "\n",
    "    num_examples:int = # How many examples do you have in the training dataset?\n",
    "\n",
    "    # That should be sufficient for training. However if you want more\n",
    "    # customization, please keep going.\n",
    "\n",
    "    training_dir:str = # Where is the training direcotory for checkpoints and logs?\n",
    "\n",
    "    regularizer = # Any weight regularization?\n",
    "\n",
    "    frequency:int = # How often do you want to log and save the model, in steps?\n",
    "\n",
    "    # All sets. Now it's time to build the model. There are two steps in ArcFace\n",
    "    # training: 1, training with softmax loss; 2, training with arcloss. This\n",
    "    # means not only different loss functions but also fragmented models.\n",
    "\n",
    "    base_model:model = # First model is base model which outputs the face embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Where is the exported model going to be saved?\n",
    "    export_dir = os.path.join(training_dir, 'exported', name)\n",
    "    \n",
    "    # Then build the second model for training.\n",
    "    if softmax:\n",
    "        print(\"Building training model with softmax loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  base_model,\n",
    "                                  keras.layers.Dense(num_ids,\n",
    "                                                     kernel_regularizer=regularizer),\n",
    "                                  keras.layers.Softmax()],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = keras.losses.CategoricalCrossentropy()\n",
    "    else:\n",
    "        print(\"Building training model with ARC loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  base_model,\n",
    "                                  L2Normalization(),\n",
    "                                  ArcLayer(num_ids, regularizer)],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = ArcLoss()\n",
    "\n",
    "    # Summary the model to find any thing suspicious at early stage.\n",
    "    model.summary()\n",
    "\n",
    "    # Construct an optimizer. This optimizer is different from the official\n",
    "    # implementation which use SGD with momentum.\n",
    "    optimizer = keras.optimizers.Adam(adam_alpha, amsgrad=True, epsilon=adam_epsilon)\n",
    "\n",
    "    # Construct training datasets.\n",
    "    dataset_train = build_dataset(train_files,\n",
    "                                  batch_size=batch_size,\n",
    "                                  one_hot_depth=num_ids,\n",
    "                                  training=True,\n",
    "                                  buffer_size=4096)\n",
    "\n",
    "    # Construct dataset for validation. The loss value from this dataset can be\n",
    "    # used to decide which checkpoint should be preserved.\n",
    "    if val_files:\n",
    "        dataset_val = build_dataset(val_files,\n",
    "                                    batch_size=batch_size,\n",
    "                                    one_hot_depth=num_ids,\n",
    "                                    training=False,\n",
    "                                    buffer_size=4096)\n",
    "    else:\n",
    "        dataset_val = None\n",
    "\n",
    "    # The training adventure is long and full of traps. A training supervisor\n",
    "    # can help us to ease the pain.\n",
    "    supervisor = TrainingSupervisor(model,\n",
    "                                    optimizer,\n",
    "                                    loss_fun,\n",
    "                                    dataset_train,\n",
    "                                    training_dir,\n",
    "                                    frequency,\n",
    "                                    \"categorical_accuracy\",\n",
    "                                    'max',\n",
    "                                    name)\n",
    "\n",
    "    # If training accomplished, save the base model for inference.\n",
    "    if export_only:\n",
    "        print(\"The best model will be exported.\")\n",
    "        supervisor.restore(restore_weights_only, True)\n",
    "        supervisor.export(base_model, export_dir)\n",
    "        quit()\n",
    "\n",
    "    # Restore the latest model if checkpoints are available.\n",
    "    supervisor.restore(restore_weights_only)\n",
    "\n",
    "    # Sometimes the training process might go wrong and we would like to resume\n",
    "    # training from manually selected checkpoint. In this case some training\n",
    "    # objects should be overridden before training started.\n",
    "    if override:\n",
    "        supervisor.override(0, 1)\n",
    "        print(\"Training process overridden by user.\")\n",
    "\n",
    "    # Now it is safe to start training.\n",
    "    supervisor.train(epochs, num_examples // batch_size)\n",
    "\n",
    "    # Export the model after training.\n",
    "    supervisor.export(base_model, export_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428e159f-666a-4fd2-9bfc-06021f71f8f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model with softmax loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_model (Functional) (None, 512)               8753922   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               307800    \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 600)               0         \n",
      "=================================================================\n",
      "Total params: 9,061,722\n",
      "Trainable params: 9,034,830\n",
      "Non-trainable params: 26,892\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a24447310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a24447310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 19:46:19.083249: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-20 19:46:19.104115: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2592010000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Checkpoint not found. Model will be initialized                 from scratch.\n",
      "Restoring..\n",
      "Only the model weights will be restored.\n",
      "Checkpoint restored: None\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;28;212;28m------------------------------------------------------------------------------------------\u001b[0m| 0/1500 [00:00<?, ?it/s]\u001b[0m2022-02-20 19:46:40.881991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-20 19:46:42.605464: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-20 19:46:48.914700: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-02-20 19:46:48.999579: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-02-20 19:47:06.013298: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-02-20 19:47:06.655281: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-02-20 19:47:07.229932: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.38GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4a24413f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4a24413f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [06:06<02:17,  3.63it/s, loss=6.26, accuracy=0.232]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.2319, mean loss: 9.83\n",
      "Monitor value improved from 0.0000 to 0.2319.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-1\n",
      "Checkpoint saved at global step: 1000, to file: outputs/checkpoints/processed_soft/ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [08:38<00:00,  2.73it/s, loss=5.99, accuracy=0.375]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.3747, mean loss: 8.44\n",
      "Monitor value improved from 0.2319 to 0.3747.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [08:41<00:00,  2.88it/s, loss=5.99, accuracy=0.375]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1500, to file: outputs/checkpoints/processed_soft/ckpt-4\n",
      "\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:17<04:39,  3.58it/s, loss=3.13, accuracy=0.472]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.4716, mean loss: 7.41\n",
      "Monitor value improved from 0.3747 to 0.4716.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-5\n",
      "Checkpoint saved at global step: 2000, to file: outputs/checkpoints/processed_soft/ckpt-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:08<00:00,  3.24it/s, loss=2.37, accuracy=0.597]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5965, mean loss: 5.98\n",
      "Monitor value improved from 0.4716 to 0.5965.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-7\n",
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_soft/ckpt-8\n",
      "Training accuracy: 0.5965, mean loss: 5.98\n",
      "Monitor value not improved: 0.5965, latest: 0.5965.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:15<00:00,  3.44it/s, loss=2.37, accuracy=0.597]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_soft/ckpt-9\n",
      "\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:48<02:12,  3.78it/s, loss=1.80, accuracy=0.674]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6738, mean loss: 5.06\n",
      "Monitor value improved from 0.5965 to 0.6738.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-10\n",
      "Checkpoint saved at global step: 4000, to file: outputs/checkpoints/processed_soft/ckpt-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:10<00:00,  3.77it/s, loss=1.79, accuracy=0.702]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7018, mean loss: 4.71\n",
      "Monitor value improved from 0.6738 to 0.7018.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:14<00:00,  3.45it/s, loss=1.79, accuracy=0.702]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 4500, to file: outputs/checkpoints/processed_soft/ckpt-13\n",
      "\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:23<05:12,  3.20it/s, loss=1.51, accuracy=0.725]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7254, mean loss: 4.42\n",
      "Monitor value improved from 0.7018 to 0.7254.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-14\n",
      "Checkpoint saved at global step: 5000, to file: outputs/checkpoints/processed_soft/ckpt-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.21it/s, loss=1.39, accuracy=0.762]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7615, mean loss: 3.94\n",
      "Monitor value improved from 0.7254 to 0.7615.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-16\n",
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_soft/ckpt-17\n",
      "Training accuracy: 0.7615, mean loss: 3.94\n",
      "Monitor value not improved: 0.7615, latest: 0.7615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:22<00:00,  3.39it/s, loss=1.39, accuracy=0.762]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_soft/ckpt-18\n",
      "\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:42<02:13,  3.74it/s, loss=1.45, accuracy=0.789]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7889, mean loss: 3.58\n",
      "Monitor value improved from 0.7615 to 0.7889.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-19\n",
      "Checkpoint saved at global step: 7000, to file: outputs/checkpoints/processed_soft/ckpt-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:08<00:00,  3.87it/s, loss=1.14, accuracy=0.799]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7990, mean loss: 3.43\n",
      "Monitor value improved from 0.7889 to 0.7990.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:12<00:00,  3.47it/s, loss=1.14, accuracy=0.799]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 7500, to file: outputs/checkpoints/processed_soft/ckpt-22\n",
      "\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:22<04:34,  3.64it/s, loss=1.09, accuracy=0.809]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8093, mean loss: 3.30\n",
      "Monitor value improved from 0.7990 to 0.8093.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-23\n",
      "Checkpoint saved at global step: 8000, to file: outputs/checkpoints/processed_soft/ckpt-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:10<00:00,  3.44it/s, loss=1.01, accuracy=0.825]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8254, mean loss: 3.07\n",
      "Monitor value improved from 0.8093 to 0.8254.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-25\n",
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_soft/ckpt-26\n",
      "Training accuracy: 0.8254, mean loss: 3.07\n",
      "Monitor value not improved: 0.8254, latest: 0.8254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:17<00:00,  3.42it/s, loss=1.01, accuracy=0.825]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_soft/ckpt-27\n",
      "\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:49<02:03,  4.04it/s, loss=1.12, accuracy=0.839]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8389, mean loss: 2.87\n",
      "Monitor value improved from 0.8254 to 0.8389.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-28\n",
      "Checkpoint saved at global step: 10000, to file: outputs/checkpoints/processed_soft/ckpt-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:13<00:00,  3.94it/s, loss=1.72, accuracy=0.845]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8446, mean loss: 2.79\n",
      "Monitor value improved from 0.8389 to 0.8446.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.43it/s, loss=1.72, accuracy=0.845]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 10500, to file: outputs/checkpoints/processed_soft/ckpt-31\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:21<04:34,  3.64it/s, loss=0.90, accuracy=0.850]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8502, mean loss: 2.71\n",
      "Monitor value improved from 0.8446 to 0.8502.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-32\n",
      "Checkpoint saved at global step: 11000, to file: outputs/checkpoints/processed_soft/ckpt-33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:17<00:00,  3.73it/s, loss=2.09, accuracy=0.860]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8602, mean loss: 2.56\n",
      "Monitor value improved from 0.8502 to 0.8602.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-34\n",
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_soft/ckpt-35\n",
      "Training accuracy: 0.8602, mean loss: 2.56\n",
      "Monitor value not improved: 0.8602, latest: 0.8602.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:23<00:00,  3.38it/s, loss=2.09, accuracy=0.860]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_soft/ckpt-36\n",
      "\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:40<02:33,  3.25it/s, loss=0.86, accuracy=0.869]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8687, mean loss: 2.44\n",
      "Monitor value improved from 0.8602 to 0.8687.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-37\n",
      "Checkpoint saved at global step: 13000, to file: outputs/checkpoints/processed_soft/ckpt-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:12<00:00,  2.77it/s, loss=0.87, accuracy=0.872]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8724, mean loss: 2.38\n",
      "Monitor value improved from 0.8687 to 0.8724.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:17<00:00,  3.43it/s, loss=0.87, accuracy=0.872]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 13500, to file: outputs/checkpoints/processed_soft/ckpt-40\n",
      "\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:22<04:15,  3.92it/s, loss=0.82, accuracy=0.876]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8762, mean loss: 2.33\n",
      "Monitor value improved from 0.8724 to 0.8762.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-41\n",
      "Checkpoint saved at global step: 14000, to file: outputs/checkpoints/processed_soft/ckpt-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:07<00:00,  3.36it/s, loss=0.73, accuracy=0.883]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8827, mean loss: 2.23\n",
      "Monitor value improved from 0.8762 to 0.8827.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-43\n",
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_soft/ckpt-44\n",
      "Training accuracy: 0.8827, mean loss: 2.23\n",
      "Monitor value not improved: 0.8827, latest: 0.8827.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:14<00:00,  3.45it/s, loss=0.73, accuracy=0.883]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_soft/ckpt-45\n",
      "\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:42<02:09,  3.85it/s, loss=0.83, accuracy=0.889]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8886, mean loss: 2.14\n",
      "Monitor value improved from 0.8827 to 0.8886.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-46\n",
      "Checkpoint saved at global step: 16000, to file: outputs/checkpoints/processed_soft/ckpt-47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:05<00:00,  2.98it/s, loss=0.79, accuracy=0.891]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8907, mean loss: 2.10\n",
      "Monitor value improved from 0.8886 to 0.8907.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:09<00:00,  3.49it/s, loss=0.79, accuracy=0.891]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 16500, to file: outputs/checkpoints/processed_soft/ckpt-49\n",
      "\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:19<04:37,  3.60it/s, loss=0.68, accuracy=0.893]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8934, mean loss: 2.06\n",
      "Monitor value improved from 0.8907 to 0.8934.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-50\n",
      "Checkpoint saved at global step: 17000, to file: outputs/checkpoints/processed_soft/ckpt-51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:17<00:00,  3.82it/s, loss=1.26, accuracy=0.898]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8978, mean loss: 1.99\n",
      "Monitor value improved from 0.8934 to 0.8978.\n",
      "Best model found and saved: outputs/model_scout/processed_soft/ckpt-52\n",
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_soft/ckpt-53\n",
      "Training accuracy: 0.8978, mean loss: 1.99\n",
      "Monitor value not improved: 0.8978, latest: 0.8978.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:24<00:00,  3.38it/s, loss=1.26, accuracy=0.898]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_soft/ckpt-54\n",
      "Training accomplished at epoch 12\n",
      "Saving model to outputs/exported/processed_soft ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-02-20 21:15:43.494610: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/exported/processed_soft/assets\n",
      "Model saved at: outputs/exported/processed_soft\n"
     ]
    }
   ],
   "source": [
    "# First model is bexported/e model which outputs the face embeddings.\n",
    "input_shape = (128, 128, 3)\n",
    "embedding_size = 512\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "base_model = hrnet_v2(input_shape=input_shape, output_size=embedding_size,\n",
    "                      width=18, trainable=True,\n",
    "                      kernel_regularizer=regularizer,\n",
    "                      name=\"embedding_model\")\n",
    "\n",
    "train(base_model, name = \"processed_soft\", train_files = \"datasets/train_processed.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 600, num_examples = 12000, training_dir = 'outputs/',\n",
    "          frequency = 1000, softmax = True, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 12, restore_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dddbb-f81b-4dec-bab4-ac2ee2993af9",
   "metadata": {},
   "source": [
    "# Load softmax model and train with softmax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "493cf1ea-809f-4ffe-a107-224f654b20ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Building training model with ARC loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_model (Functional) (None, 512)               8753922   \n",
      "_________________________________________________________________\n",
      "l2_normalization (L2Normaliz (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "arc_layer (ArcLayer)         (None, 600)               307200    \n",
      "=================================================================\n",
      "Total params: 9,061,122\n",
      "Trainable params: 9,034,230\n",
      "Non-trainable params: 26,892\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a00458160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a00458160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: Checkpoint not found. Model will be initialized                 from scratch.\n",
      "Restoring..\n",
      "Checkpoint restored: None\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 1000/1500 [05:17<02:38,  3.15it/s, loss=30.24, accuracy=0.627]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6265, mean loss: 31.03\n",
      "Monitor value improved from 0.0000 to 0.6265.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-1\n",
      "Checkpoint saved at global step: 1000, to file: outputs/checkpoints/processed_arc/ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:41<00:00,  3.82it/s, loss=17.15, accuracy=0.721]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7206, mean loss: 27.51\n",
      "Monitor value improved from 0.6265 to 0.7206.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:45<00:00,  3.22it/s, loss=17.15, accuracy=0.721]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1500, to file: outputs/checkpoints/processed_arc/ckpt-4\n",
      "\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 500/1500 [02:21<04:53,  3.40it/s, loss=11.76, accuracy=0.781]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7812, mean loss: 24.97\n",
      "Monitor value improved from 0.7206 to 0.7812.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-5\n",
      "Checkpoint saved at global step: 2000, to file: outputs/checkpoints/processed_arc/ckpt-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.83it/s, loss=9.85, accuracy=0.849]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8487, mean loss: 21.01\n",
      "Monitor value improved from 0.7812 to 0.8487.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-7\n",
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_arc/ckpt-8\n",
      "Training accuracy: 0.8487, mean loss: 21.01\n",
      "Monitor value not improved: 0.8487, latest: 0.8487.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:27<00:00,  3.35it/s, loss=9.85, accuracy=0.849]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_arc/ckpt-9\n",
      "\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 1000/1500 [04:31<02:33,  3.25it/s, loss=11.69, accuracy=0.884]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8842, mean loss: 18.36\n",
      "Monitor value improved from 0.8487 to 0.8842.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-10\n",
      "Checkpoint saved at global step: 4000, to file: outputs/checkpoints/processed_arc/ckpt-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:01<00:00,  3.52it/s, loss=10.02, accuracy=0.896]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8958, mean loss: 17.35\n",
      "Monitor value improved from 0.8842 to 0.8958.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:05<00:00,  3.52it/s, loss=10.02, accuracy=0.896]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 4500, to file: outputs/checkpoints/processed_arc/ckpt-13\n",
      "\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:27<04:39,  3.58it/s, loss=5.18, accuracy=0.906]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9057, mean loss: 16.49\n",
      "Monitor value improved from 0.8958 to 0.9057.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-14\n",
      "Checkpoint saved at global step: 5000, to file: outputs/checkpoints/processed_arc/ckpt-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:11<00:00,  3.42it/s, loss=6.69, accuracy=0.920]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9204, mean loss: 15.13\n",
      "Monitor value improved from 0.9057 to 0.9204.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-16\n",
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_arc/ckpt-17\n",
      "Training accuracy: 0.9204, mean loss: 15.13\n",
      "Monitor value not improved: 0.9204, latest: 0.9204.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:20<00:00,  3.40it/s, loss=6.69, accuracy=0.920]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_arc/ckpt-18\n",
      "\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:51<02:46,  3.00it/s, loss=6.44, accuracy=0.931]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9314, mean loss: 14.05\n",
      "Monitor value improved from 0.9204 to 0.9314.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-19\n",
      "Checkpoint saved at global step: 7000, to file: outputs/checkpoints/processed_arc/ckpt-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:15<00:00,  3.58it/s, loss=10.55, accuracy=0.935]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9355, mean loss: 13.61\n",
      "Monitor value improved from 0.9314 to 0.9355.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.42it/s, loss=10.55, accuracy=0.935]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 7500, to file: outputs/checkpoints/processed_arc/ckpt-22\n",
      "\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:16<04:08,  4.03it/s, loss=3.71, accuracy=0.939]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9393, mean loss: 13.20\n",
      "Monitor value improved from 0.9355 to 0.9393.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-23\n",
      "Checkpoint saved at global step: 8000, to file: outputs/checkpoints/processed_arc/ckpt-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:08<00:00,  3.66it/s, loss=6.30, accuracy=0.946]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9456, mean loss: 12.51\n",
      "Monitor value improved from 0.9393 to 0.9456.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-25\n",
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_arc/ckpt-26\n",
      "Training accuracy: 0.9456, mean loss: 12.51\n",
      "Monitor value not improved: 0.9456, latest: 0.9456.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.44it/s, loss=6.30, accuracy=0.946]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_arc/ckpt-27\n",
      "\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:43<02:18,  3.60it/s, loss=6.79, accuracy=0.951]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9508, mean loss: 11.91\n",
      "Monitor value improved from 0.9456 to 0.9508.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-28\n",
      "Checkpoint saved at global step: 10000, to file: outputs/checkpoints/processed_arc/ckpt-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:01<00:00,  3.75it/s, loss=4.69, accuracy=0.953]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9529, mean loss: 11.65\n",
      "Monitor value improved from 0.9508 to 0.9529.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:06<00:00,  3.52it/s, loss=4.69, accuracy=0.953]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 10500, to file: outputs/checkpoints/processed_arc/ckpt-31\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:28<05:03,  3.29it/s, loss=7.27, accuracy=0.955]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9549, mean loss: 11.41\n",
      "Monitor value improved from 0.9529 to 0.9549.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-32\n",
      "Checkpoint saved at global step: 11000, to file: outputs/checkpoints/processed_arc/ckpt-33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.57it/s, loss=6.33, accuracy=0.958]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9584, mean loss: 10.97\n",
      "Monitor value improved from 0.9549 to 0.9584.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-34\n",
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_arc/ckpt-35\n",
      "Training accuracy: 0.9584, mean loss: 10.97\n",
      "Monitor value not improved: 0.9584, latest: 0.9584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:24<00:00,  3.38it/s, loss=6.33, accuracy=0.958]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_arc/ckpt-36\n",
      "\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:52<02:15,  3.70it/s, loss=5.22, accuracy=0.961]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9614, mean loss: 10.58\n",
      "Monitor value improved from 0.9584 to 0.9614.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-37\n",
      "Checkpoint saved at global step: 13000, to file: outputs/checkpoints/processed_arc/ckpt-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:22<00:00,  3.43it/s, loss=9.88, accuracy=0.963]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9626, mean loss: 10.40\n",
      "Monitor value improved from 0.9614 to 0.9626.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:26<00:00,  3.36it/s, loss=9.88, accuracy=0.963]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 13500, to file: outputs/checkpoints/processed_arc/ckpt-40\n",
      "\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:19<04:26,  3.76it/s, loss=5.14, accuracy=0.964]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9639, mean loss: 10.23\n",
      "Monitor value improved from 0.9626 to 0.9639.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-41\n",
      "Checkpoint saved at global step: 14000, to file: outputs/checkpoints/processed_arc/ckpt-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:10<00:00,  3.81it/s, loss=6.52, accuracy=0.966]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9661, mean loss: 9.93\n",
      "Monitor value improved from 0.9639 to 0.9661.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-43\n",
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_arc/ckpt-44\n",
      "Training accuracy: 0.9661, mean loss: 9.93\n",
      "Monitor value not improved: 0.9661, latest: 0.9661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:18<00:00,  3.42it/s, loss=6.52, accuracy=0.966]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_arc/ckpt-45\n",
      "\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:40<02:05,  3.97it/s, loss=4.90, accuracy=0.968]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9681, mean loss: 9.64\n",
      "Monitor value improved from 0.9661 to 0.9681.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-46\n",
      "Checkpoint saved at global step: 16000, to file: outputs/checkpoints/processed_arc/ckpt-47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:02<00:00,  3.80it/s, loss=4.41, accuracy=0.969]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9689, mean loss: 9.51\n",
      "Monitor value improved from 0.9681 to 0.9689.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:06<00:00,  3.52it/s, loss=4.41, accuracy=0.969]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 16500, to file: outputs/checkpoints/processed_arc/ckpt-49\n",
      "\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:26<04:32,  3.67it/s, loss=6.54, accuracy=0.970]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9698, mean loss: 9.38\n",
      "Monitor value improved from 0.9689 to 0.9698.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-50\n",
      "Checkpoint saved at global step: 17000, to file: outputs/checkpoints/processed_arc/ckpt-51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:12<00:00,  3.91it/s, loss=3.93, accuracy=0.971]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9713, mean loss: 9.15\n",
      "Monitor value improved from 0.9698 to 0.9713.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-52\n",
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_arc/ckpt-53\n",
      "Training accuracy: 0.9713, mean loss: 9.15\n",
      "Monitor value not improved: 0.9713, latest: 0.9713.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.41it/s, loss=3.93, accuracy=0.971]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_arc/ckpt-54\n",
      "Training accomplished at epoch 12\n",
      "Saving model to outputs/exported/processed_arc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/exported/processed_arc/assets\n",
      "Model saved at: outputs/exported/processed_arc\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = 'outputs/exported/processed_soft/'\n",
    "num_ids = 600\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "\n",
    "base_model = keras.models.load_model(checkpoint_dir)\n",
    "\n",
    "\n",
    "\n",
    "train(base_model, name = \"processed_arc\", train_files = \"datasets/train_processed.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 600, num_examples = 12000, training_dir = 'outputs/',\n",
    "          frequency = 1000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 12, restore_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e0a7fb-2c4b-42c5-bd8c-55527b37ddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.models.load_model('outputs/exported/processed_arc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7b4db0-8cf7-4529-a1d6-5f750adbe80c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.05367756e+00, -9.90905881e-01, -2.78063536e+00,  2.08470440e+00,\n",
       "       -6.89761162e+00, -7.36667871e+00, -3.39260364e+00,  1.34441814e+01,\n",
       "       -4.82636929e-01,  1.13399992e+01, -6.95179892e+00, -1.19924097e+01,\n",
       "        7.88302517e+00, -1.41918335e+01, -4.11912346e+00, -1.46957254e+00,\n",
       "       -7.96988773e+00, -1.25587597e+01, -3.30817413e+00, -6.44409084e+00,\n",
       "        1.22559929e+01,  5.35371399e+00,  5.92604065e+00,  6.71351433e+00,\n",
       "       -1.11080599e+00, -2.78792906e+00, -2.90345979e+00,  2.45971823e+00,\n",
       "       -6.26247358e+00, -1.03179255e+01,  8.88810825e+00, -1.41048241e+01,\n",
       "        4.53196001e+00,  5.65346956e+00, -5.67095137e+00, -1.42811990e+00,\n",
       "        2.07321930e+00, -9.27039719e+00,  7.42131090e+00,  1.54729605e+01,\n",
       "        4.39252949e+00, -3.40530872e+00, -1.70784206e+01, -1.34897213e+01,\n",
       "       -2.26406813e+00,  1.14141998e+01,  8.65560341e+00,  1.77637482e+00,\n",
       "        3.90383005e+00, -1.00459528e+01, -7.04687500e+00,  3.19938278e+00,\n",
       "       -1.33050752e+00,  5.29237151e-01, -9.29395103e+00,  3.40610337e+00,\n",
       "        3.36477280e+00,  1.06135931e+01, -1.35090714e+01,  1.95123367e+01,\n",
       "        5.41750193e-01, -2.80407162e+01,  6.13505554e+00,  1.21662121e+01,\n",
       "       -4.23454905e+00,  3.28556204e+00,  2.52228498e-01, -7.14629555e+00,\n",
       "       -2.81993556e+00,  6.85230064e+00,  6.72806644e+00,  1.06639233e+01,\n",
       "       -4.24940538e+00,  3.38336372e+00, -4.91865158e+00, -7.66393900e+00,\n",
       "        8.25955868e+00, -5.79863358e+00,  9.47322655e+00,  8.99248695e+00,\n",
       "       -6.85396194e+00,  8.09639740e+00, -1.10271931e+01, -1.35141497e+01,\n",
       "        9.66149712e+00,  7.45807457e+00,  5.39133549e-01,  2.53944302e+00,\n",
       "       -2.27662396e+00, -7.01859045e+00,  3.89281988e+00,  7.17935038e+00,\n",
       "       -7.97067308e+00, -1.50179987e+01, -3.54539156e+00,  1.13801432e+00,\n",
       "        3.14410400e+00, -3.32093859e+00,  6.29394627e+00, -1.11261740e+01,\n",
       "        5.63355589e+00, -1.01674709e+01, -3.99779296e+00, -7.74366558e-01,\n",
       "       -4.03720093e+00, -7.20428753e+00,  5.40167761e+00, -2.71579409e+00,\n",
       "        5.35092449e+00, -6.59607553e+00,  4.36505699e+00,  8.29977036e+00,\n",
       "        1.78383827e-01, -2.43249106e+00, -1.06458855e+01,  1.37207019e+00,\n",
       "        4.99649620e+00,  1.59486878e+00,  5.90793753e+00, -3.90097308e+00,\n",
       "       -1.48512182e+01, -8.92228889e+00,  3.67731452e+00,  8.73648071e+00,\n",
       "       -5.36905766e+00, -2.17814040e+00,  2.30672026e+00,  8.35988903e+00,\n",
       "       -3.23023272e+00,  1.09388332e+01, -8.81460667e+00, -9.50673342e-01,\n",
       "       -5.34245539e+00,  8.77865219e+00,  1.12528744e+01,  9.39970016e+00,\n",
       "        1.45609617e+00, -4.70030975e+00,  1.17288723e+01,  1.84407005e+01,\n",
       "       -1.56424170e+01, -7.78996658e+00, -9.78327084e+00,  1.71242952e-01,\n",
       "       -1.80775719e+01,  1.53603554e+00, -4.96954203e+00,  9.92212772e+00,\n",
       "       -2.13716674e+00, -7.78097010e+00, -7.11381865e+00, -8.62040043e+00,\n",
       "        6.53267288e+00,  1.21355724e+01,  8.57726574e+00, -2.66762352e+00,\n",
       "        8.03038025e+00,  1.10360107e+01, -8.34419632e+00,  7.61761665e-01,\n",
       "       -1.01201391e+01,  1.98623836e+00,  7.59404898e-01,  8.37977314e+00,\n",
       "        5.04899025e-01,  4.92880154e+00,  2.08245010e+01,  1.13122044e+01,\n",
       "        7.40290546e+00, -1.03679523e+01,  4.46195221e+00,  4.13633060e+00,\n",
       "        7.22499609e-01, -1.94523144e+00, -1.10039282e+01, -5.97673130e+00,\n",
       "       -1.66453362e+00, -9.35829735e+00,  3.37126780e+00,  1.09828215e+01,\n",
       "       -1.22807817e+01,  7.63496017e+00,  2.29131870e+01, -4.80568504e+00,\n",
       "       -2.85098004e+00, -1.75167694e+01,  1.39805937e+01,  7.33983850e+00,\n",
       "       -6.92531586e+00, -7.51374054e+00, -1.05168486e+01, -8.91650081e-01,\n",
       "       -1.01729679e+01, -5.96404076e-02,  2.46792960e+00,  8.37346363e+00,\n",
       "        3.96021605e+00, -3.89099383e+00,  3.45784521e+00, -4.37528753e+00,\n",
       "       -8.17600346e+00,  1.86619987e+01,  2.21069050e+00,  6.23869228e+00,\n",
       "        1.49200954e+01,  9.86367226e+00, -8.02228355e+00, -5.11302900e+00,\n",
       "        1.24111176e-01, -7.19007969e-01,  5.74355030e+00,  9.66036034e+00,\n",
       "       -2.81533957e+00, -8.79203892e+00,  1.15830688e+01,  2.45898104e+00,\n",
       "        2.99522161e-01, -1.45887518e+01,  1.00001554e+01,  5.53752422e-01,\n",
       "        9.96437740e+00,  1.21644354e+01, -1.56281929e+01, -1.15587072e+01,\n",
       "        8.24397564e+00, -8.29158306e-01,  9.22394562e+00,  4.46152210e-01,\n",
       "       -2.07984424e+00,  1.53624792e+01, -9.26179790e+00,  2.58909750e+00,\n",
       "        2.00453949e+00,  1.79235897e+01,  1.23748837e+01, -3.26748848e-01,\n",
       "       -9.87306213e+00, -7.10600567e+00,  1.03761053e+00,  6.64239025e+00,\n",
       "       -5.96942377e+00,  8.61536026e+00,  1.58714986e+00, -1.71831970e+01,\n",
       "        1.49640408e+01,  8.25040340e+00, -5.51690149e+00, -1.54669027e+01,\n",
       "        1.60090942e+01, -2.21681747e+01,  6.05823040e+00, -3.89776278e+00,\n",
       "        8.76019955e+00, -6.97920609e+00,  1.98408108e+01,  1.97481003e+01,\n",
       "       -7.74614239e+00,  9.27596569e+00, -1.07363868e+00, -1.14432802e+01,\n",
       "       -1.31860008e+01, -9.96334076e+00,  1.05108624e+01,  1.03516445e+01,\n",
       "       -2.49583340e+00,  6.99823475e+00, -9.71713066e-01, -1.24237919e+00,\n",
       "       -7.61501026e+00, -1.60779023e+00,  1.34209275e+00, -1.66961918e+01,\n",
       "        9.48915482e+00,  1.52508554e+01,  9.89356613e+00, -1.33184891e+01,\n",
       "        3.79326248e+00, -4.56538010e+00, -7.06736469e+00, -4.93363523e+00,\n",
       "        1.16350508e+01, -6.22526360e+00, -3.35845947e+00, -1.58434944e+01,\n",
       "       -3.47151780e+00,  1.79527783e+00,  1.18242073e+00, -4.22121048e+00,\n",
       "       -2.17314529e+00, -6.66281700e-01, -1.71446457e+01,  3.52386546e+00,\n",
       "       -1.08717508e+01, -1.43281746e+00,  1.39985571e+01,  9.10268307e-01,\n",
       "        2.82825851e+00, -1.07910490e+01, -2.02163410e+01, -1.80550766e+00,\n",
       "       -1.49184294e+01,  1.12686920e+00, -5.52223730e+00, -1.13013496e+01,\n",
       "       -5.96312046e+00,  6.27079201e+00, -5.02019215e+00,  4.51594639e+00,\n",
       "        7.87879944e+00, -5.38829136e+00,  5.61927986e+00,  3.94453859e+00,\n",
       "       -7.66734600e+00, -3.01885557e+00,  8.13264370e+00, -5.93511963e+00,\n",
       "        1.83272686e+01,  2.20823193e+00,  6.77788544e+00,  3.05972099e-01,\n",
       "        1.75055733e+01, -1.36971045e+01,  8.05770111e+00,  9.69837189e-01,\n",
       "       -4.53516626e+00,  3.18596745e+00,  4.52610540e+00,  1.45523393e+00,\n",
       "        5.00314713e-01, -1.03064184e+01,  1.64681149e+01,  1.22419567e+01,\n",
       "        1.42788544e+01,  8.64001942e+00,  2.81011319e+00,  2.45450735e+00,\n",
       "       -7.33810854e+00, -2.10795736e+00,  5.40217590e+00, -1.65689611e+00,\n",
       "       -3.71258402e+00, -5.08905029e+00, -6.32589054e+00,  1.04822655e+01,\n",
       "        1.57002001e+01,  1.58167171e+01, -1.93571873e+01,  2.50378656e+00,\n",
       "        3.19022083e+00, -4.39990187e+00,  1.69698086e+01, -6.67886925e+00,\n",
       "        1.49334021e+01,  1.06953678e+01,  4.22523165e+00,  1.61600018e+00,\n",
       "        4.20988655e+00,  1.14195042e+01,  3.84739351e+00, -1.84993577e+00,\n",
       "        5.96564674e+00,  4.97957945e+00, -2.10241675e+00, -4.82241535e+00,\n",
       "        1.01511469e+01,  8.71463299e+00, -6.88675261e+00,  6.73809052e+00,\n",
       "       -2.17611265e+00, -8.14941406e+00, -3.56795502e+00,  1.53056021e+01,\n",
       "       -4.91599751e+00,  2.15422940e+00, -8.84133720e+00, -1.00167751e+01,\n",
       "       -8.36579037e+00,  8.61201000e+00,  7.49168301e+00,  9.33539772e+00,\n",
       "        8.87908554e+00,  1.02991219e+01, -7.21500540e+00,  1.70756664e+01,\n",
       "       -3.32674026e+00, -4.16345644e+00,  1.17696877e+01, -6.77152061e+00,\n",
       "       -1.38074093e+01,  6.30320978e+00, -1.94260287e+00,  2.44605517e+00,\n",
       "        4.04261780e+00, -6.30940771e+00, -8.31992149e-01,  7.61636305e+00,\n",
       "       -4.90405607e+00,  7.24150658e+00, -2.99434781e+00,  6.88107109e+00,\n",
       "        1.13498545e+01,  4.76662636e+00,  6.93643093e-01, -1.21670694e+01,\n",
       "        1.54776554e+01, -4.80346251e+00, -9.54806519e+00,  5.81493378e-01,\n",
       "       -1.36377549e+00,  3.95107174e+00, -3.63849640e-01,  9.05045509e-01,\n",
       "        1.30063114e+01,  2.72713280e+00, -1.18131170e+01,  5.60193968e+00,\n",
       "        9.52984810e-01, -9.45432663e+00,  8.82128716e+00,  9.77363396e+00,\n",
       "        6.42718554e+00, -6.09581709e+00,  1.15511436e+01,  2.08152351e+01,\n",
       "       -1.57126474e+00,  6.59164476e+00, -7.88134480e+00,  1.11378069e+01,\n",
       "       -1.63763046e+01,  3.08044100e+00,  5.66922903e-01, -5.76584339e+00,\n",
       "       -1.43359547e+01, -3.89838791e+00,  6.58092880e+00, -1.23757572e+01,\n",
       "        3.48267126e+00, -9.95829391e+00,  7.17009020e+00, -4.36942339e+00,\n",
       "        1.32393389e+01,  1.17911301e+01, -1.76015224e+01, -4.65323544e+00,\n",
       "        2.29976273e+00, -1.21511250e+01,  1.65667801e+01,  2.49050140e-01,\n",
       "        1.30293684e+01, -1.04430437e+01,  5.70303631e+00, -5.95985842e+00,\n",
       "       -2.47608280e+00, -7.80205309e-01, -3.55102491e+00,  8.25369072e+00,\n",
       "        2.54913712e+00,  4.29020548e+00, -7.47417307e+00, -7.58636427e+00,\n",
       "        2.77097034e+00, -2.73711252e+00,  6.26549578e+00,  1.59033995e+01,\n",
       "        6.21256399e+00, -3.55567074e+00, -1.44807606e+01, -1.64879084e-01,\n",
       "        4.33313131e-01, -1.46735945e+01,  1.00224075e+01, -5.20969677e+00,\n",
       "        1.35279961e+01, -2.24909210e+01,  1.69289112e+01,  6.39226151e+00,\n",
       "        9.15451050e-02, -6.98725891e+00, -2.46806526e+00, -7.51832485e+00,\n",
       "        5.22890043e+00,  9.96568108e+00, -1.58805108e+00, -7.06031322e-01,\n",
       "       -1.38073289e+00,  3.62714267e+00,  1.11574221e+00,  4.99395037e+00,\n",
       "       -4.51329172e-01, -1.07263174e+01, -9.20064545e+00,  2.55167150e+00,\n",
       "       -7.12753248e+00, -6.50394964e+00,  3.99788857e+00,  2.00950336e+00,\n",
       "       -8.46786499e-01,  1.47049963e+00, -9.85408115e+00, -3.68072462e+00,\n",
       "       -2.73632050e+00,  1.70931816e-02,  1.36777620e+01, -1.66919117e+01,\n",
       "        1.19212437e+00,  3.52026796e+00, -1.72544074e+00,  8.57016087e+00,\n",
       "       -1.02114944e+01,  4.98987293e+00,  3.67512321e+00, -2.55497723e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = glob('datasets/processed/*/*.bmp')\n",
    "img = cv2.imread(samples[1]).reshape(-1,128,128,3)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.001, amsgrad=True, epsilon=0.001)\n",
    "# loss_fun = keras.losses.CategoricalCrossentropy()\n",
    "# base_model.compile(optimizer, loss_fun)\n",
    "\n",
    "base_model.predict(img)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2381e95-27ed-4513-97c0-5b919c5db94c",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9627a76d-c6ed-4869-92d0-ead611b6ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(pkl, path = 'model.pkl'):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(pkl, f)\n",
    "    print(\"saved pkl file at:\",path)\n",
    "\n",
    "def load_pkl(path='model.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        pkl = pickle.load(f)\n",
    "    return pkl\n",
    "\n",
    "def make_embeddings(dataset_path='datasets/sorted_palmvein_roi/', output_path='outputs/', model_dir='outputs/exported/arcface'):\n",
    "    # Grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying palms...\")\n",
    "    imagePaths = glob(dataset_path+'/*/*.bmp')\n",
    "#     imagePaths.extend(glob(dataset_path+'/*/*.jpg'))\n",
    "\n",
    "    # Initialize model\n",
    "    embedding_model = keras.models.load_model(model_dir)\n",
    "       \n",
    "    # Initialize our lists of extracted facial embeddings and corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # Initialize the total number of faces processed\n",
    "    total = 0\n",
    "\n",
    "    # Loop over the imagePaths\n",
    "    for (i, imagePath) in tqdm(enumerate(imagePaths)):\n",
    "        # extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "         # load the image\n",
    "        img = cv2.imread(imagePath).reshape(-1,128,128,3)\n",
    "        palms_embedding = embedding_model.predict(img)[0]\n",
    "        # add the name of the person + corresponding face\n",
    "        # embedding to their respective list\n",
    "        knownNames.append(name)\n",
    "        knownEmbeddings.append(palms_embedding)\n",
    "        total += 1\n",
    "        \n",
    "    print(total, \" palms embedded\")\n",
    "#     print(set(knownNames))\n",
    "\n",
    "    # save to output\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    save_pkl(pkl=data, path=output_path+'db.pkl')\n",
    "    \n",
    "def make_model(embeddings_path='outputs/db.pkl', output_path='outputs/'):\n",
    "    # Load the face embeddings\n",
    "    data = load_pkl(embeddings_path)\n",
    "    num_classes = len(np.unique(data[\"names\"]))\n",
    "    y = np.array(data[\"names\"])\n",
    "    X = np.array(data[\"embeddings\"])\n",
    "    \n",
    "    \n",
    "    # Initialize Softmax training model arguments\n",
    "    input_shape = X.shape[1]\n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "    model =  MLPClassifier(hidden_layer_sizes=(input_shape, 640, 112, 640, num_classes), activation='tanh',max_iter=10000, batch_size='auto', learning_rate='adaptive',\n",
    "                           validation_fraction=0.0, solver='adam', early_stopping=False ,verbose=0,random_state=1)\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx],)\n",
    "        print(model.score(X[valid_idx], y[valid_idx]), end='\\t')\n",
    "    \n",
    "    save_pkl(pkl=model, path=output_path+'model.pkl')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c00b650-1ae4-4681-896c-118fa718fbe4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying palms...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12000it [13:35, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000  palms embedded\n"
     ]
    }
   ],
   "source": [
    "make_embeddings(dataset_path='datasets/processed/', output_path='outputs/exported/processed_arc/', model_dir='outputs/exported/processed_arc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca291fa-161b-4827-8d3a-67efb436ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5154166666666666\t0.6154166666666666\t0.5870833333333333\t0.5979166666666667\t0.49\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(512, 640, 112, 640, 600),\n",
       "              learning_rate='adaptive', max_iter=10000, random_state=1,\n",
       "              validation_fraction=0.0, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model(embeddings_path='outputs/exported/processed_arc/db.pkl', output_path='outputs/exported/processed_arc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff4da8a9-c786-47d2-8974-208ea1d20d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 12000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13859/2823632888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/palm/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \"\"\"\n\u001b[0;32m-> 1599\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/palm/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/palm/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 12000]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "data = load_pkl('outputs/db.pkl')\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=600, random_state = 7)\n",
    "split = splitter.split(data, groups=data[\"names\"])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b6399bb-ab4f-48c0-86ce-1249cab3c4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(512,)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i.shape for i in data[\"embeddings\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f24d2fed-0111-4668-b3e3-7b878e7e4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6da1e3-c41b-4b45-894a-2bbd58b19464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm",
   "language": "python",
   "name": "palm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
