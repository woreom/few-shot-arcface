{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56270576-6b91-44c0-b748-2669a3459d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"../arcface/\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from arcface.dataset import build_dataset\n",
    "from arcface.losses import ArcLoss\n",
    "from arcface.network import ArcLayer, L2Normalization, hrnet_v2\n",
    "from arcface.training_supervisor import TrainingSupervisor\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "\n",
    "import IPython.display as display\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e51799-16e5-42fb-8357-a6b222a3dcf5",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "1. hrnet (Done)\n",
    "2. resnet ()\n",
    "3. efficienthrnet\n",
    "4. efficientnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec79e2-e72c-4b4f-a95c-062781b88a26",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c655c8ad-aca2-444a-b455-e1c04897b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_example(image, label):\n",
    "    feature = {\n",
    "        \"image\": image_feature(image),\n",
    "        \"label\": bytes_feature(label),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "### Backbone Class make (ToDo Make into a separate module)############################\n",
    "class RecordOperator(ABC):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        # Construct a reader if the user is trying to read the record file.\n",
    "        self.dataset = None\n",
    "        self._writer = None\n",
    "        if tf.io.gfile.exists(filename):\n",
    "            self.dataset = tf.data.TFRecordDataset(filename)\n",
    "        else:\n",
    "            # Construct a writer in case the user want to write something.\n",
    "            self._writer = tf.io.TFRecordWriter(filename)\n",
    "\n",
    "        # Set the feature description. This should be provided before trying to\n",
    "        # parse the record file.\n",
    "        self.set_feature_description()\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_example(self):\n",
    "        \"\"\"Returns a tf.train.example from values to be saved.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def write_example(self, tf_example):\n",
    "        \"\"\"Create TFRecord example from a data sample.\"\"\"\n",
    "        if self._writer is None:\n",
    "            raise IOError(\"Record file already exists.\")\n",
    "        else:\n",
    "            self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_feature_description(self):\n",
    "        \"\"\"Set the feature_description to parse TFRecord file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def parse_dataset(self):\n",
    "        # Create a dictionary describing the features. This dict should be\n",
    "        # consistent with the one used while generating the record file.\n",
    "        if self.dataset is None:\n",
    "            raise IOError(\"Dataset file not found.\")\n",
    "\n",
    "        def _parse_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            return tf.io.parse_single_example(example_proto, self.feature_description)\n",
    "\n",
    "        parsed_dataset = self.dataset.map(_parse_function)\n",
    "        return parsed_dataset\n",
    "#########################################################################################\n",
    "\n",
    "class ImageDataset(RecordOperator):\n",
    "    \"\"\"Construct ImageDataset tfrecord files.\"\"\"\n",
    "\n",
    "    def make_example(self, image, label):\n",
    "        \"\"\"Construct an tf.Example with image data and label.\n",
    "        Args:\n",
    "            image_string: encoded image, NOT as numpy array.\n",
    "            label: the label.\n",
    "        Returns:\n",
    "            a tf.Example.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_string = tf.image.decode_image(image)\n",
    "        image_shape = image_string.shape\n",
    "        \n",
    "\n",
    "        # After getting all the features, time to generate a TensorFlow example.\n",
    "        feature = {\n",
    "            'image/height': int64_feature(image_shape[0]),\n",
    "            'image/width': int64_feature(image_shape[1]),\n",
    "            'image/depth': int64_feature(image_shape[2]),\n",
    "            'image/encoded': image_feature(image_string),\n",
    "            'label': int64_feature(label),\n",
    "        }\n",
    "\n",
    "        tf_example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "\n",
    "        return tf_example\n",
    "    \n",
    "    def set_feature_description(self):\n",
    "        self.feature_description = {\n",
    "            \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/encoded\": tf.io.VarLenFeature(tf.float32),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "#         example = tf.io.parse_single_example(example, feature_description)\n",
    "#         example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "#         return example\n",
    "\n",
    "\n",
    "def create_tfrecord(path = 'datasets/sorted_palmvein_roi/',  types=[\".bmp\"], tf_record='datasets/train.record'):\n",
    "    \n",
    "    converter = ImageDataset(tf_record)\n",
    "    samples =[]\n",
    "    [samples.extend(glob(path+\"*/*.\"+typ)) for typ in types]\n",
    "    total_samples_num = len(samples)\n",
    "    ids = set()\n",
    "    print(\"Total records: {}\".format(total_samples_num))\n",
    "    \n",
    "    for i, image_path in tqdm(enumerate(samples)):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        ids.add(image_path.split('/')[-2])\n",
    "        label = len(ids)\n",
    "        tf_example = converter.make_example(image, label)\n",
    "        # Write the example to file.\n",
    "        converter.write_example(tf_example)\n",
    "        \n",
    "    print(\"All done. Record file is:\\n{}\".format(tf_record))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abbc9f2-9561-4be1-941a-858e4540f80f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 30902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30902it [00:15, 2044.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Record file is:\n",
      "datasets/merge_processed.record\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name='merge'\n",
    "create_tfrecord(path= 'datasets/processed_merge/', types=['png','jpg','jepg','png', 'bmp'], tf_record='datasets/'+name+'_processed.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa326f-43af-4dbb-890a-531af22b71a1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fdd0f44-d827-43b0-b3fe-321d94100ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_model, name = \"hrnetv2\", train_files = \"datasets/train.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 600, num_examples = 12000, training_dir = os.getcwd(),\n",
    "          frequency = 1000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 50, restore_weights_only=False):\n",
    "\n",
    "    '''\n",
    "    # Deep neural network training is complicated. The first thing is making\n",
    "    # sure you have everything ready for training, like datasets, checkpoints,\n",
    "    # logs, etc. Modify these paths to suit your needs.\n",
    "\n",
    "    name:str = # What is the model's name?\n",
    "    \n",
    "    train_files:str = # Where are the training files?\n",
    "\n",
    "    test_files:str = # Where are the testing files?\n",
    "\n",
    "    val_files:str = # Where are the validation files? Set `None` if no files available.\n",
    "\n",
    "    input_shape:tuple(int) = # What is the shape of the input image?\n",
    "\n",
    "    embedding_size:int = # What is the size of the embeddings that represent the faces?\n",
    "\n",
    "    num_ids:int = # How many identities do you have in the training dataset?\n",
    "\n",
    "    num_examples:int = # How many examples do you have in the training dataset?\n",
    "\n",
    "    # That should be sufficient for training. However if you want more\n",
    "    # customization, please keep going.\n",
    "\n",
    "    training_dir:str = # Where is the training direcotory for checkpoints and logs?\n",
    "\n",
    "    regularizer = # Any weight regularization?\n",
    "\n",
    "    frequency:int = # How often do you want to log and save the model, in steps?\n",
    "\n",
    "    # All sets. Now it's time to build the model. There are two steps in ArcFace\n",
    "    # training: 1, training with softmax loss; 2, training with arcloss. This\n",
    "    # means not only different loss functions but also fragmented models.\n",
    "\n",
    "    base_model:model = # First model is base model which outputs the face embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Where is the exported model going to be saved?\n",
    "    export_dir = os.path.join(training_dir, 'exported', name)\n",
    "    \n",
    "    # Then build the second model for training.\n",
    "    if softmax:\n",
    "        print(\"Building training model with softmax loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  base_model,\n",
    "                                  keras.layers.Dense(num_ids,\n",
    "                                                     kernel_regularizer=regularizer),\n",
    "                                  keras.layers.Softmax()],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = keras.losses.CategoricalCrossentropy()\n",
    "    else:\n",
    "        print(\"Building training model with ARC loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  base_model,\n",
    "                                  L2Normalization(),\n",
    "                                  ArcLayer(num_ids, regularizer)],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = ArcLoss()\n",
    "\n",
    "    # Summary the model to find any thing suspicious at early stage.\n",
    "    model.summary()\n",
    "\n",
    "    # Construct an optimizer. This optimizer is different from the official\n",
    "    # implementation which use SGD with momentum.\n",
    "    optimizer = keras.optimizers.Adam(adam_alpha, amsgrad=True, epsilon=adam_epsilon)\n",
    "\n",
    "    # Construct training datasets.\n",
    "    dataset_train = build_dataset(train_files,\n",
    "                                  batch_size=batch_size,\n",
    "                                  one_hot_depth=num_ids,\n",
    "                                  training=True,\n",
    "                                  buffer_size=4096)\n",
    "\n",
    "    # Construct dataset for validation. The loss value from this dataset can be\n",
    "    # used to decide which checkpoint should be preserved.\n",
    "    if val_files:\n",
    "        dataset_val = build_dataset(val_files,\n",
    "                                    batch_size=batch_size,\n",
    "                                    one_hot_depth=num_ids,\n",
    "                                    training=False,\n",
    "                                    buffer_size=4096)\n",
    "\n",
    "    # The training adventure is long and full of traps. A training supervisor\n",
    "    # can help us to ease the pain.\n",
    "    supervisor = TrainingSupervisor(model,\n",
    "                                    optimizer,\n",
    "                                    loss_fun,\n",
    "                                    dataset_train,\n",
    "                                    training_dir,\n",
    "                                    frequency,\n",
    "                                    \"categorical_accuracy\",\n",
    "                                    'max',\n",
    "                                    name)\n",
    "\n",
    "    # If training accomplished, save the base model for inference.\n",
    "    if export_only:\n",
    "        print(\"The best model will be exported.\")\n",
    "        supervisor.restore(restore_weights_only, True)\n",
    "        supervisor.export(base_model, export_dir)\n",
    "        quit()\n",
    "\n",
    "    # Restore the latest model if checkpoints are available.\n",
    "    supervisor.restore(restore_weights_only)\n",
    "\n",
    "    # Sometimes the training process might go wrong and we would like to resume\n",
    "    # training from manually selected checkpoint. In this case some training\n",
    "    # objects should be overridden before training started.\n",
    "    if override:\n",
    "        supervisor.override(0, 1)\n",
    "        print(\"Training process overridden by user.\")\n",
    "\n",
    "    # Now it is safe to start training.\n",
    "    supervisor.train(epochs, num_examples // batch_size)\n",
    "\n",
    "    # Export the model after training.\n",
    "    supervisor.export(base_model, export_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e159f-666a-4fd2-9bfc-06021f71f8f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:34:54.173627: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-18 14:34:54.174651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-18 14:34:54.224418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-05-18 14:34:54.224468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-18 14:34:54.226293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-18 14:34:54.226352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-18 14:34:54.228001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-18 14:34:54.228315: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-18 14:34:54.230273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-18 14:34:54.231349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-18 14:34:54.235494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-18 14:34:54.237095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-18 14:34:54.237729: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-18 14:34:54.238846: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-18 14:34:54.239664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-05-18 14:34:54.239723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-18 14:34:54.239755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-18 14:34:54.239784: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-18 14:34:54.239812: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-18 14:34:54.239840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-18 14:34:54.239868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-18 14:34:54.239896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-18 14:34:54.239925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-18 14:34:54.241436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-18 14:34:54.241489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-18 14:34:54.793322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-18 14:34:54.793353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-05-18 14:34:54.793377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-05-18 14:34:54.795382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10011 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model with softmax loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_model (Functional) (None, 512)               8753922   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1144)              586872    \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 1144)              0         \n",
      "=================================================================\n",
      "Total params: 9,340,794\n",
      "Trainable params: 9,313,902\n",
      "Non-trainable params: 26,892\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f810f9cc820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f810f9cc820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Checkpoint found: outputs/checkpoints/merge_soft/ckpt-19\n",
      "Restoring..\n",
      "Only the model weights will be restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 14:35:00.483081: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-18 14:35:00.503940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499940000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored: outputs/checkpoints/merge_soft/ckpt-19\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;28;212;28m-------------------------------------------------------------------------------------------\u001b[0m| 0/483 [00:00<?, ?it/s]\u001b[0m2022-05-18 14:35:26.298159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-18 14:35:26.507480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-18 14:35:27.946954: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.85GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f810fc4fc10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f810fc4fc10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:08<00:00,  1.87it/s, loss=6.18, accuracy=0.396]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.3960, mean loss: 9.93\n",
      "Monitor value improved from 0.0000 to 0.3960.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:12<00:00,  1.91it/s, loss=6.18, accuracy=0.396]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 482, to file: outputs/checkpoints/merge_soft/ckpt-2\n",
      "\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:27<00:00,  1.74it/s, loss=4.25, accuracy=0.606]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6061, mean loss: 7.89\n",
      "Monitor value improved from 0.3960 to 0.6061.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:30<00:00,  1.78it/s, loss=4.25, accuracy=0.606]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 964, to file: outputs/checkpoints/merge_soft/ckpt-4\n",
      "\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|\u001b[38;2;28;212;28m>>>>---------------------------------------------------\u001b[0m| 36/483 [00:18<04:16,  1.74it/s, loss=4.68, accuracy=0.614]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6139, mean loss: 7.78\n",
      "Monitor value improved from 0.6061 to 0.6139.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-5\n",
      "Checkpoint saved at global step: 1000, to file: outputs/checkpoints/merge_soft/ckpt-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:41<00:00,  1.68it/s, loss=2.99, accuracy=0.724]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7236, mean loss: 6.52\n",
      "Monitor value improved from 0.6139 to 0.7236.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:45<00:00,  1.69it/s, loss=2.99, accuracy=0.724]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1446, to file: outputs/checkpoints/merge_soft/ckpt-8\n",
      "\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:47<00:00,  1.66it/s, loss=2.13, accuracy=0.787]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7872, mean loss: 5.55\n",
      "Monitor value improved from 0.7236 to 0.7872.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:50<00:00,  1.66it/s, loss=2.13, accuracy=0.787]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1928, to file: outputs/checkpoints/merge_soft/ckpt-10\n",
      "\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|\u001b[38;2;28;212;28m>>>>>>>>-----------------------------------------------\u001b[0m| 72/483 [00:41<04:08,  1.65it/s, loss=2.27, accuracy=0.792]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7924, mean loss: 5.43\n",
      "Monitor value improved from 0.7872 to 0.7924.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-11\n",
      "Checkpoint saved at global step: 2000, to file: outputs/checkpoints/merge_soft/ckpt-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:51<00:00,  1.63it/s, loss=1.61, accuracy=0.826]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8257, mean loss: 4.82\n",
      "Monitor value improved from 0.7924 to 0.8257.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:54<00:00,  1.64it/s, loss=1.61, accuracy=0.826]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2410, to file: outputs/checkpoints/merge_soft/ckpt-14\n",
      "\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:53<00:00,  1.62it/s, loss=1.21, accuracy=0.852]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8517, mean loss: 4.27\n",
      "Monitor value improved from 0.8257 to 0.8517.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:57<00:00,  1.62it/s, loss=1.21, accuracy=0.852]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2892, to file: outputs/checkpoints/merge_soft/ckpt-16\n",
      "\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|\u001b[38;2;28;212;28m>>>>>>>>>>>>------------------------------------------\u001b[0m| 108/483 [01:03<03:50,  1.62it/s, loss=1.28, accuracy=0.855]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8550, mean loss: 4.16\n",
      "Monitor value improved from 0.8517 to 0.8550.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-17\n",
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/merge_soft/ckpt-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:55<00:00,  1.63it/s, loss=0.99, accuracy=0.870]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8704, mean loss: 3.82\n",
      "Monitor value improved from 0.8550 to 0.8704.\n",
      "Best model found and saved: outputs/model_scout/merge_soft/ckpt-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 482/483 [04:59<00:00,  1.61it/s, loss=0.99, accuracy=0.870]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 3374, to file: outputs/checkpoints/merge_soft/ckpt-20\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|\u001b[38;2;28;212;28m>>>>>>>------------------------------------------------\u001b[0m| 67/483 [00:39<04:15,  1.63it/s, loss=1.09, accuracy=0.871]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# First model is bexported/e model which outputs the face embeddings.\n",
    "input_shape = (128, 128, 3)\n",
    "embedding_size = 512\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "base_model = hrnet_v2(input_shape=input_shape, output_size=embedding_size,\n",
    "                           trainable=True,\n",
    "                           kernel_regularizer=regularizer,\n",
    "                           name=\"embedding_model\")\n",
    "\n",
    "name='merge'\n",
    "train(base_model, name = name+\"_soft\", train_files = \"datasets/\"+name+\"_processed.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 1144, num_examples = 30942, training_dir = 'outputs/',\n",
    "          frequency = 1000, softmax = True, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 64, export_only = False,\n",
    "          override = False, epochs = 12, restore_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dddbb-f81b-4dec-bab4-ac2ee2993af9",
   "metadata": {},
   "source": [
    "# Load softmax model and train with softmax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "493cf1ea-809f-4ffe-a107-224f654b20ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Building training model with ARC loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_model (Functional) (None, 512)               8753922   \n",
      "_________________________________________________________________\n",
      "l2_normalization (L2Normaliz (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "arc_layer (ArcLayer)         (None, 600)               307200    \n",
      "=================================================================\n",
      "Total params: 9,061,122\n",
      "Trainable params: 9,034,230\n",
      "Non-trainable params: 26,892\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a00458160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f4a00458160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: Checkpoint not found. Model will be initialized                 from scratch.\n",
      "Restoring..\n",
      "Checkpoint restored: None\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 1000/1500 [05:17<02:38,  3.15it/s, loss=30.24, accuracy=0.627]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6265, mean loss: 31.03\n",
      "Monitor value improved from 0.0000 to 0.6265.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-1\n",
      "Checkpoint saved at global step: 1000, to file: outputs/checkpoints/processed_arc/ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:41<00:00,  3.82it/s, loss=17.15, accuracy=0.721]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7206, mean loss: 27.51\n",
      "Monitor value improved from 0.6265 to 0.7206.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:45<00:00,  3.22it/s, loss=17.15, accuracy=0.721]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1500, to file: outputs/checkpoints/processed_arc/ckpt-4\n",
      "\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 500/1500 [02:21<04:53,  3.40it/s, loss=11.76, accuracy=0.781]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7812, mean loss: 24.97\n",
      "Monitor value improved from 0.7206 to 0.7812.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-5\n",
      "Checkpoint saved at global step: 2000, to file: outputs/checkpoints/processed_arc/ckpt-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.83it/s, loss=9.85, accuracy=0.849]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8487, mean loss: 21.01\n",
      "Monitor value improved from 0.7812 to 0.8487.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-7\n",
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_arc/ckpt-8\n",
      "Training accuracy: 0.8487, mean loss: 21.01\n",
      "Monitor value not improved: 0.8487, latest: 0.8487.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:27<00:00,  3.35it/s, loss=9.85, accuracy=0.849]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 3000, to file: outputs/checkpoints/processed_arc/ckpt-9\n",
      "\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 1000/1500 [04:31<02:33,  3.25it/s, loss=11.69, accuracy=0.884]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8842, mean loss: 18.36\n",
      "Monitor value improved from 0.8487 to 0.8842.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-10\n",
      "Checkpoint saved at global step: 4000, to file: outputs/checkpoints/processed_arc/ckpt-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:01<00:00,  3.52it/s, loss=10.02, accuracy=0.896]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8958, mean loss: 17.35\n",
      "Monitor value improved from 0.8842 to 0.8958.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:05<00:00,  3.52it/s, loss=10.02, accuracy=0.896]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 4500, to file: outputs/checkpoints/processed_arc/ckpt-13\n",
      "\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:27<04:39,  3.58it/s, loss=5.18, accuracy=0.906]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9057, mean loss: 16.49\n",
      "Monitor value improved from 0.8958 to 0.9057.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-14\n",
      "Checkpoint saved at global step: 5000, to file: outputs/checkpoints/processed_arc/ckpt-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:11<00:00,  3.42it/s, loss=6.69, accuracy=0.920]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9204, mean loss: 15.13\n",
      "Monitor value improved from 0.9057 to 0.9204.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-16\n",
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_arc/ckpt-17\n",
      "Training accuracy: 0.9204, mean loss: 15.13\n",
      "Monitor value not improved: 0.9204, latest: 0.9204.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:20<00:00,  3.40it/s, loss=6.69, accuracy=0.920]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 6000, to file: outputs/checkpoints/processed_arc/ckpt-18\n",
      "\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:51<02:46,  3.00it/s, loss=6.44, accuracy=0.931]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9314, mean loss: 14.05\n",
      "Monitor value improved from 0.9204 to 0.9314.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-19\n",
      "Checkpoint saved at global step: 7000, to file: outputs/checkpoints/processed_arc/ckpt-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:15<00:00,  3.58it/s, loss=10.55, accuracy=0.935]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9355, mean loss: 13.61\n",
      "Monitor value improved from 0.9314 to 0.9355.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.42it/s, loss=10.55, accuracy=0.935]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 7500, to file: outputs/checkpoints/processed_arc/ckpt-22\n",
      "\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:16<04:08,  4.03it/s, loss=3.71, accuracy=0.939]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9393, mean loss: 13.20\n",
      "Monitor value improved from 0.9355 to 0.9393.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-23\n",
      "Checkpoint saved at global step: 8000, to file: outputs/checkpoints/processed_arc/ckpt-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:08<00:00,  3.66it/s, loss=6.30, accuracy=0.946]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9456, mean loss: 12.51\n",
      "Monitor value improved from 0.9393 to 0.9456.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-25\n",
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_arc/ckpt-26\n",
      "Training accuracy: 0.9456, mean loss: 12.51\n",
      "Monitor value not improved: 0.9456, latest: 0.9456.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.44it/s, loss=6.30, accuracy=0.946]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 9000, to file: outputs/checkpoints/processed_arc/ckpt-27\n",
      "\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:43<02:18,  3.60it/s, loss=6.79, accuracy=0.951]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9508, mean loss: 11.91\n",
      "Monitor value improved from 0.9456 to 0.9508.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-28\n",
      "Checkpoint saved at global step: 10000, to file: outputs/checkpoints/processed_arc/ckpt-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:01<00:00,  3.75it/s, loss=4.69, accuracy=0.953]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9529, mean loss: 11.65\n",
      "Monitor value improved from 0.9508 to 0.9529.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:06<00:00,  3.52it/s, loss=4.69, accuracy=0.953]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 10500, to file: outputs/checkpoints/processed_arc/ckpt-31\n",
      "\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:28<05:03,  3.29it/s, loss=7.27, accuracy=0.955]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9549, mean loss: 11.41\n",
      "Monitor value improved from 0.9529 to 0.9549.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-32\n",
      "Checkpoint saved at global step: 11000, to file: outputs/checkpoints/processed_arc/ckpt-33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:16<00:00,  3.57it/s, loss=6.33, accuracy=0.958]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9584, mean loss: 10.97\n",
      "Monitor value improved from 0.9549 to 0.9584.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-34\n",
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_arc/ckpt-35\n",
      "Training accuracy: 0.9584, mean loss: 10.97\n",
      "Monitor value not improved: 0.9584, latest: 0.9584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:24<00:00,  3.38it/s, loss=6.33, accuracy=0.958]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 12000, to file: outputs/checkpoints/processed_arc/ckpt-36\n",
      "\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:52<02:15,  3.70it/s, loss=5.22, accuracy=0.961]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9614, mean loss: 10.58\n",
      "Monitor value improved from 0.9584 to 0.9614.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-37\n",
      "Checkpoint saved at global step: 13000, to file: outputs/checkpoints/processed_arc/ckpt-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:22<00:00,  3.43it/s, loss=9.88, accuracy=0.963]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9626, mean loss: 10.40\n",
      "Monitor value improved from 0.9614 to 0.9626.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:26<00:00,  3.36it/s, loss=9.88, accuracy=0.963]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 13500, to file: outputs/checkpoints/processed_arc/ckpt-40\n",
      "\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:19<04:26,  3.76it/s, loss=5.14, accuracy=0.964]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9639, mean loss: 10.23\n",
      "Monitor value improved from 0.9626 to 0.9639.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-41\n",
      "Checkpoint saved at global step: 14000, to file: outputs/checkpoints/processed_arc/ckpt-42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:10<00:00,  3.81it/s, loss=6.52, accuracy=0.966]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9661, mean loss: 9.93\n",
      "Monitor value improved from 0.9639 to 0.9661.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-43\n",
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_arc/ckpt-44\n",
      "Training accuracy: 0.9661, mean loss: 9.93\n",
      "Monitor value not improved: 0.9661, latest: 0.9661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:18<00:00,  3.42it/s, loss=6.52, accuracy=0.966]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 15000, to file: outputs/checkpoints/processed_arc/ckpt-45\n",
      "\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 1000/1500 [04:40<02:05,  3.97it/s, loss=4.90, accuracy=0.968]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9681, mean loss: 9.64\n",
      "Monitor value improved from 0.9661 to 0.9681.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-46\n",
      "Checkpoint saved at global step: 16000, to file: outputs/checkpoints/processed_arc/ckpt-47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:02<00:00,  3.80it/s, loss=4.41, accuracy=0.969]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9689, mean loss: 9.51\n",
      "Monitor value improved from 0.9681 to 0.9689.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:06<00:00,  3.52it/s, loss=4.41, accuracy=0.969]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 16500, to file: outputs/checkpoints/processed_arc/ckpt-49\n",
      "\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 500/1500 [02:26<04:32,  3.67it/s, loss=6.54, accuracy=0.970]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9698, mean loss: 9.38\n",
      "Monitor value improved from 0.9689 to 0.9698.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-50\n",
      "Checkpoint saved at global step: 17000, to file: outputs/checkpoints/processed_arc/ckpt-51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:12<00:00,  3.91it/s, loss=3.93, accuracy=0.971]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9713, mean loss: 9.15\n",
      "Monitor value improved from 0.9698 to 0.9713.\n",
      "Best model found and saved: outputs/model_scout/processed_arc/ckpt-52\n",
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_arc/ckpt-53\n",
      "Training accuracy: 0.9713, mean loss: 9.15\n",
      "Monitor value not improved: 0.9713, latest: 0.9713.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1500/1500 [07:19<00:00,  3.41it/s, loss=3.93, accuracy=0.971]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 18000, to file: outputs/checkpoints/processed_arc/ckpt-54\n",
      "Training accomplished at epoch 12\n",
      "Saving model to outputs/exported/processed_arc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/exported/processed_arc/assets\n",
      "Model saved at: outputs/exported/processed_arc\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = 'outputs/exported/processed_soft/'\n",
    "num_ids = 600\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "\n",
    "base_model = keras.models.load_model(checkpoint_dir)\n",
    "\n",
    "\n",
    "name='merge'\n",
    "train(base_model, name = name+\"_arc\", train_files = \"datasets/\"+name+\"_processed.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 1144, num_examples = 30942, training_dir = 'outputs/',\n",
    "          frequency = 1000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 12, restore_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2381e95-27ed-4513-97c0-5b919c5db94c",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9627a76d-c6ed-4869-92d0-ead611b6ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(pkl, path = 'model.pkl'):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(pkl, f)\n",
    "    print(\"saved pkl file at:\",path)\n",
    "\n",
    "def load_pkl(path='model.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        pkl = pickle.load(f)\n",
    "    return pkl\n",
    "\n",
    "def findCosineDistance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculate cosine distance between two vector\n",
    "    \"\"\"\n",
    "    vec1 = vector1.flatten()\n",
    "    vec2 = vector2.flatten()\n",
    "\n",
    "    a = np.dot(vec1.T, vec2)\n",
    "    b = np.dot(vec1.T, vec1)\n",
    "    c = np.dot(vec2.T, vec2)\n",
    "    return 1 - (a/(np.sqrt(b)*np.sqrt(c)))\n",
    "\n",
    "def cosine_similarity(test_vec, source_vecs):\n",
    "    \"\"\"\n",
    "    Verify the similarity of one vector to group vectors of one class\n",
    "    \"\"\"\n",
    "    cos_dist = 0\n",
    "    for source_vec in source_vecs:\n",
    "        cos_dist += findCosineDistance(test_vec, source_vec)\n",
    "    return cos_dist/len(source_vecs)\n",
    "\n",
    "def make_embeddings(dataset_path='datasets/sorted_palmvein_roi/', output_path='outputs/', model_dir='outputs/exported/arcface', types='bmp'):\n",
    "    # Grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying palms...\")\n",
    "    imagePaths =[]\n",
    "    [imagePaths.extend(glob(dataset_path+\"*/*.\"+typ)) for typ in types]\n",
    "#     imagePaths.extend(glob(dataset_path+'/*/*.jpg'))\n",
    "\n",
    "    # Initialize model\n",
    "    embedding_model = keras.models.load_model(model_dir)\n",
    "       \n",
    "    # Initialize our lists of extracted facial embeddings and corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # Initialize the total number of faces processed\n",
    "    total = 0\n",
    "\n",
    "    # Loop over the imagePaths\n",
    "    for (i, imagePath) in tqdm(enumerate(imagePaths)):\n",
    "        # extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "#         print(imagePath)\n",
    "\n",
    "         # load the image\n",
    "        img = cv2.imread(imagePath).reshape(-1,128,128,3)\n",
    "        palms_embedding = embedding_model.predict(img)[0]\n",
    "        # add the name of the person + corresponding face\n",
    "        # embedding to their respective list\n",
    "        knownNames.append(name)\n",
    "        knownEmbeddings.append(palms_embedding)\n",
    "        total += 1\n",
    "        \n",
    "    print(total, \" palms embedded\")\n",
    "#     print(set(knownNames))\n",
    "\n",
    "    # save to output\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    save_pkl(pkl=data, path=output_path+'db.pkl')\n",
    "    \n",
    "def make_model(embeddings_path='outputs/db.pkl', output_path='outputs/'):\n",
    "    # Load the face embeddings\n",
    "    data = load_pkl(embeddings_path)\n",
    "    num_classes = len(np.unique(data[\"names\"]))\n",
    "    y = np.array(data[\"names\"])\n",
    "    X = np.array(data[\"embeddings\"])\n",
    "    \n",
    "    \n",
    "    # Initialize Softmax training model arguments\n",
    "    input_shape = X.shape[1]\n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "    model =  MLPClassifier(hidden_layer_sizes=(input_shape, 640, 112, 640, num_classes), activation='tanh',max_iter=10000, batch_size='auto', learning_rate='adaptive',\n",
    "                           validation_fraction=0.0, solver='adam', early_stopping=False ,verbose=0,random_state=1)\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx],)\n",
    "        print(model.score(X[valid_idx], y[valid_idx]), end='\\t')\n",
    "    \n",
    "    save_pkl(pkl=model, path=output_path+'model.pkl')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c00b650-1ae4-4681-896c-118fa718fbe4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying palms...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:04, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49  palms embedded\n",
      "saved pkl file at: outputs/exported/processed_resnet_arc/db.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "make_embeddings(dataset_path='datasets/processed_merge/', output_path='outputs/exported/'+name+'_arc/', model_dir='outputs/exported/'+name+'_arc/',types=['png','jpg','jepg','png', 'bmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ca291fa-161b-4827-8d3a-67efb436ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\t1.0\t0.7\t0.7\t0.4444444444444444\tsaved pkl file at: outputs/exported/processed_resnet_arc/model.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(512, 640, 112, 640, 5),\n",
       "              learning_rate='adaptive', max_iter=10000, random_state=1,\n",
       "              validation_fraction=0.0, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_model(embeddings_path='outputs/exported/'+name+'_arc/db.pkl', output_path='outputs/exported/'+name+'_arc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17f1fba4-736d-4895-8fb6-6ac165fc357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = keras.models.load_model('outputs/exported/'+name+'_arc/')\n",
    "model = load_pkl('outputs/exported/'+name+'_arc/model.pkl')\n",
    "samples = glob('datasets/processed_merge/*/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff4da8a9-c786-47d2-8974-208ea1d20d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957\t0.778\t0.738\t0.679\t0.690\t0.694\t0.735\t0.788\t0.901\t0.948\t0.815\t\n",
      "ali-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tamir-Left Hand\tali-Left Hand\tahmad-Right Hand\t\n",
      "ahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Left Hand\tahmad-Right Hand\t\n"
     ]
    }
   ],
   "source": [
    "img_paths = samples[0:11]\n",
    "imgs = [cv2.imread(img).reshape(-1, 128, 128, 3) for img in img_paths]\n",
    "embedding = [embedding_model.predict(img)[0] for img in imgs]\n",
    "\n",
    "[print(np.format_float_positional((cosine_similarity(i, embedding[:11])), precision=3), end='\\t') for i in embedding]\n",
    "print()\n",
    "[print(i, end='\\t') for i in model.predict(embedding)]\n",
    "print()\n",
    "[print(img.split(\"/\")[-2], end='\\t') for img in img_paths] \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a56e6-f000-438b-8c52-90fa40b0b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm",
   "language": "python",
   "name": "palm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
