{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56270576-6b91-44c0-b748-2669a3459d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"../arcface/\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from arcface.dataset import build_dataset\n",
    "from arcface.losses import ArcLoss\n",
    "from arcface.network import ArcLayer, L2Normalization, resnet101\n",
    "from arcface.training_supervisor import TrainingSupervisor\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "\n",
    "import IPython.display as display\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2577a951-ed1d-4e9f-b98f-ff794ec36237",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For cpu only\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244d6cf0-bc0f-4421-b429-1d32a8366afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e51799-16e5-42fb-8357-a6b222a3dcf5",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "1. hrnet (Done)\n",
    "2. resnet ()\n",
    "3. efficienthrnet\n",
    "4. efficientnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994fce1-9f9b-4089-a0a7-b4f57a2948ed",
   "metadata": {},
   "source": [
    "# Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af278af2-3a9f-4fe5-b6cb-ac81ad041b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def plot(images):\n",
    "    from matplotlib import pyplot as plt\n",
    "    n=len(images)\n",
    "    fig, axes = plt.subplots(ncols=n, figsize=(16, 5))\n",
    "    ax = axes.ravel()\n",
    "    for i in range(1,n+1):\n",
    "        ax[i-1] = plt.subplot(1, n, i)\n",
    "        ax[i-1].imshow(images[i-1])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# reduce noise in the image\n",
    "def reduce_noise(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    noise = cv2.fastNlMeansDenoising(gray)\n",
    "    return cv2.cvtColor(noise, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# histogram equalization\n",
    "def equalize_hist(img, kernel=np.ones((7,7),np.uint8)):\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# invert a binary image\n",
    "def invert(img):\n",
    "    return cv2.bitwise_not(img)\n",
    "\n",
    "def inverted(img):\n",
    "    return (255-img)\n",
    "\n",
    "# gray\n",
    "def gray(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# skeletonize the image\n",
    "def skel(gray):\n",
    "    img = gray.copy()\n",
    "    skel = img.copy()\n",
    "    skel[:,:] = 0\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (5,5))\n",
    "\n",
    "    while cv2.countNonZero(img) != 0:\n",
    "        eroded = cv2.morphologyEx(img, cv2.MORPH_ERODE, kernel)\n",
    "        temp = cv2.morphologyEx(eroded, cv2.MORPH_DILATE, kernel)\n",
    "        temp = cv2.subtract(img, temp)\n",
    "        skel = cv2.bitwise_or(skel, temp)\n",
    "        img[:,:] = eroded[:,:]\n",
    "    return skel\n",
    "\n",
    "# threshold to make the veins more visible\n",
    "def thresh(img):\n",
    "    _, thr = cv2.threshold(img, 5,255, cv2.THRESH_BINARY)\n",
    "    return thr\n",
    "\n",
    "def Sift(img, skeleton):\n",
    "    scale = cv2.SIFT_create()\n",
    "    kp = scale.detect(skeleton,None)\n",
    "    img_copy=img.copy()\n",
    "    img_copy=cv2.drawKeypoints(skeleton,kp,img_copy)\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "def background_substraction_1(img):\n",
    "    #equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=20.0, tileGridSize=(8,8))\n",
    "    img= clahe.apply(img)\n",
    "    CAP2 = img.copy()\n",
    "    #Filter medianBlur\n",
    "    CAP3 = cv2.medianBlur(img,5)\n",
    "    \n",
    "    return CAP3\n",
    "\n",
    "def background_substraction_2(img):\n",
    "    #equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=20.0, tileGridSize=(8,8))\n",
    "    img= clahe.apply(img)\n",
    "    CAP2 = img.copy()\n",
    "    #Binarización\n",
    "    ret, CAP4 = cv2.threshold(CAP2,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    CAP4 = 255-CAP4.copy()\n",
    "\n",
    "    return CAP4\n",
    "\n",
    "\n",
    "def ridge(img, ridge_filter=cv2.ximgproc.RidgeDetectionFilter_create()):\n",
    "    ridges = ridge_filter.getRidgeFilteredImage(img)\n",
    "    return ridges\n",
    "    \n",
    "def process_image(imgIn, imgOut=None):\n",
    "#     print(\"processing...\")\n",
    "    img = cv2.imread(imgIn)\n",
    "    kernel = np.ones((7,7),np.uint8)\n",
    "    noise = reduce_noise(img)\n",
    "    img_output = equalize_hist(noise, kernel)\n",
    "    inv = invert(img_output)\n",
    "    gray_scale = gray(inv)\n",
    "#     print(\"skeletonizing...\")\n",
    "    skeleton = skel(gray_scale)\n",
    "    thr = thresh(skeleton)\n",
    "    scale = Sift(img, skeleton)\n",
    "\n",
    "#     cv2.imwrite(imgOut, thr)\n",
    "#     plot([img, img_output, gray_scale, skeleton, thr, scale])\n",
    "#     print(\"done\")\n",
    "\n",
    "    return thr\n",
    "\n",
    "def resize(img, dim=(128,128)):\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def proc_maker(list_func, img_path):\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    proc_img = [img]\n",
    "    for func in list_func:\n",
    "        proc_img.append(func(proc_img[-1]))\n",
    "        \n",
    "#     plot(proc_img)\n",
    "    \n",
    "    return proc_img[-1]\n",
    "\n",
    "def preprocess(procs, dataset_path='datasets/sorted_palmvein_roi/', types='bmp', output_path='datasets/processed/'):\n",
    "    imagePaths =[]\n",
    "    [imagePaths.extend(glob(dataset_path+'/*/*.'+image_type)) for image_type in types]\n",
    "    make_folder(output_path)\n",
    "    flag=set()\n",
    "    for img_path in tqdm(imagePaths):\n",
    "        folder_name = img_path.split('/')[-2]\n",
    "        name = img_path.split('/')[-1]\n",
    "        img =  proc_maker(procs, img_path)\n",
    "        if folder_name in flag:\n",
    "            cv2.imwrite(output_path+folder_name+'/'+name, img)\n",
    "        else:\n",
    "            make_folder(output_path+folder_name)\n",
    "            cv2.imwrite(output_path+folder_name+'/'+name, img)\n",
    "            flag.add(folder_name)\n",
    "            \n",
    "            \n",
    "def augment_data(path = 'datasets/sorted_palmvein_roi/',  types=[\".bmp\"], dataset_path='datasets/augmented/', aug_num=12):\n",
    "    \n",
    "    data_augmentation = A.Compose([\n",
    "            A.Rotate(limit=(0,360), p=1, border_mode=0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1),\n",
    "            # A.ShiftScaleRotate(scale_limit=0.0, shift_limit_x=0, shift_limit_y=0)\n",
    "        ])\n",
    "    \n",
    "    samples =[]\n",
    "    [samples.extend(glob(path+\"*/*.\"+typ)) for typ in types]\n",
    "    \n",
    "    for i, image_path in tqdm(enumerate(samples)):\n",
    "        \n",
    "        label = image_path.split('/')[-2]\n",
    "        \n",
    "        name = image_path.split('/')[-1]\n",
    "        filename, _type= name.split(\".\")\n",
    "        \n",
    "        directory = dataset_path+\"/\"+label\n",
    "        make_folder(directory)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (128,128))\n",
    "        cv2.imwrite(directory+\"/\"+name, image)\n",
    "        \n",
    "        for j in range(1, aug_num):\n",
    "            cv2.imwrite(directory+\"/\"+f\"{filename}_{j}.{_type}\", data_augmentation(image=image)[\"image\"])\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2442f7d-42b6-4b98-865c-905aab02ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18000it [08:37, 34.81it/s]\n"
     ]
    }
   ],
   "source": [
    "name='600_unprocessed'\n",
    "augment_data(path= 'datasets/'+name+\"/\", types=['png','jpg','jepg','png', 'bmp'], dataset_path='datasets/600+500_aug/', aug_num=9)\n",
    "\n",
    "name='500_unprocessed'\n",
    "augment_data(path= 'datasets/'+name+\"/\", types=['png','jpg','jepg','png', 'bmp'], dataset_path='datasets/600+500_aug/', aug_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a5f3a-db82-43db-9dab-1514b06fc246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▋                                                                | 8100/198000 [04:16<1:38:54, 32.00it/s]"
     ]
    }
   ],
   "source": [
    "ALL_PROCS = [reduce_noise, equalize_hist, invert, gray, skel, thresh, Sift, background_substraction_1, background_substraction_2, ridge, inverted, resize]\n",
    "proc=[reduce_noise, equalize_hist, gray, invert, skel, background_substraction_2, invert, resize]\n",
    "\n",
    "preprocess(proc, dataset_path='datasets/600+500_aug', types=['png','jpg','jepg','png', 'bmp'], output_path='datasets/600+500_processed/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec79e2-e72c-4b4f-a95c-062781b88a26",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c655c8ad-aca2-444a-b455-e1c04897b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def create_example(image, label):\n",
    "    feature = {\n",
    "        \"image\": image_feature(image),\n",
    "        \"label\": bytes_feature(label),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "### Backbone Class make (ToDo Make into a separate module)############################\n",
    "class RecordOperator(ABC):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        # Construct a reader if the user is trying to read the record file.\n",
    "        self.dataset = None\n",
    "        self._writer = None\n",
    "        if tf.io.gfile.exists(filename):\n",
    "            self.dataset = tf.data.TFRecordDataset(filename)\n",
    "        else:\n",
    "            # Construct a writer in case the user want to write something.\n",
    "            self._writer = tf.io.TFRecordWriter(filename)\n",
    "\n",
    "        # Set the feature description. This should be provided before trying to\n",
    "        # parse the record file.\n",
    "        self.set_feature_description()\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_example(self):\n",
    "        \"\"\"Returns a tf.train.example from values to be saved.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def write_example(self, tf_example):\n",
    "        \"\"\"Create TFRecord example from a data sample.\"\"\"\n",
    "        if self._writer is None:\n",
    "            raise IOError(\"Record file already exists.\")\n",
    "        else:\n",
    "            self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_feature_description(self):\n",
    "        \"\"\"Set the feature_description to parse TFRecord file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def parse_dataset(self):\n",
    "        # Create a dictionary describing the features. This dict should be\n",
    "        # consistent with the one used while generating the record file.\n",
    "        if self.dataset is None:\n",
    "            raise IOError(\"Dataset file not found.\")\n",
    "\n",
    "        def _parse_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            return tf.io.parse_single_example(example_proto, self.feature_description)\n",
    "\n",
    "        parsed_dataset = self.dataset.map(_parse_function)\n",
    "        return parsed_dataset\n",
    "#########################################################################################\n",
    "\n",
    "class ImageDataset(RecordOperator):\n",
    "    \"\"\"Construct ImageDataset tfrecord files.\"\"\"\n",
    "\n",
    "    def make_example(self, image, label):\n",
    "        \"\"\"Construct an tf.Example with image data and label.\n",
    "        Args:\n",
    "            image_string: encoded image, NOT as numpy array.\n",
    "            label: the label.\n",
    "        Returns:\n",
    "            a tf.Example.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_string = tf.image.decode_image(image)\n",
    "        image_shape = image_string.shape\n",
    "        \n",
    "\n",
    "        # After getting all the features, time to generate a TensorFlow example.\n",
    "        feature = {\n",
    "            'image/height': int64_feature(image_shape[0]),\n",
    "            'image/width': int64_feature(image_shape[1]),\n",
    "            'image/depth': int64_feature(image_shape[2]),\n",
    "            'image/encoded': image_feature(image_string),\n",
    "            'label': int64_feature(label),\n",
    "        }\n",
    "\n",
    "        tf_example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature))\n",
    "\n",
    "        return tf_example\n",
    "    \n",
    "    def set_feature_description(self):\n",
    "        self.feature_description = {\n",
    "            \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"image/encoded\": tf.io.VarLenFeature(tf.float32),\n",
    "            \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "#         example = tf.io.parse_single_example(example, feature_description)\n",
    "#         example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "#         return example\n",
    "\n",
    "def split_tfrecord_creator(split_samples, split_tf_record):\n",
    "        converter = ImageDataset(split_tf_record)\n",
    "        total_samples_num = len(split_samples)\n",
    "        ids = set()\n",
    "\n",
    "        for i, image_path in enumerate(split_samples):\n",
    "            image = tf.io.read_file(image_path)\n",
    "            ids.add(image_path.split('/')[-2])\n",
    "            label = len(ids)\n",
    "            tf_example = converter.make_example(image, label)\n",
    "            # Write the example to file.\n",
    "            converter.write_example(tf_example)\n",
    "\n",
    "        print(f\"Total num_ids: {len(ids)}, Total num_examples: {total_samples_num}\")\n",
    "        print(\"All done. Record file is:\\n{}\".format(split_tf_record))\n",
    "        \n",
    "\n",
    "def create_tfrecord(path = 'datasets/sorted_palmvein_roi/',  types=[\".bmp\"], tf_record='datasets/train.record', test_size=0.2, random_state=42):\n",
    "    samples =[]\n",
    "    [samples.extend(glob(path+\"*/*.\"+typ)) for typ in types]\n",
    "    labels = [image_path.split('/')[-2] for image_path in samples]\n",
    "    sample_labels = [image_path.split('/')[-2] for image_path in samples]\n",
    "    samples_train, samples_val = train_test_split(samples, test_size=test_size, random_state=random_state, stratify=sample_labels)\n",
    "    \n",
    "    split_tfrecord_creator(split_samples= samples_train, split_tf_record= tf_record+\"_train.record\")\n",
    "    split_tfrecord_creator(split_samples= samples_val, split_tf_record= tf_record+\"_val.record\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abbc9f2-9561-4be1-941a-858e4540f80f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 20:51:12.953048: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-27 20:51:12.960758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-27 20:51:13.327428: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.327656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.86GiB/s\n",
      "2022-07-27 20:51:13.327806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-27 20:51:13.342503: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-27 20:51:13.342750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-27 20:51:13.348830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-27 20:51:13.349853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-27 20:51:13.363440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-27 20:51:13.368630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-27 20:51:13.385222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-27 20:51:13.386125: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.386852: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.386978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-27 20:51:13.389096: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 20:51:13.391393: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-27 20:51:13.392172: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.392269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.86GiB/s\n",
      "2022-07-27 20:51:13.392433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-27 20:51:13.392566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-27 20:51:13.392658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-27 20:51:13.392687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-27 20:51:13.392710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-27 20:51:13.392799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-27 20:51:13.392886: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-27 20:51:13.392915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-27 20:51:13.393479: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.394106: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:13.394126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-27 20:51:13.394237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-27 20:51:14.330752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-27 20:51:14.330777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-27 20:51:14.330786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-27 20:51:14.331584: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:14.331606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-07-27 20:51:14.332102: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:14.332676: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 20:51:14.332782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4709 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num_ids: 1142, Total num_examples: 24721\n",
      "All done. Record file is:\n",
      "datasets/processed_merge_train.record\n",
      "Total num_ids: 1142, Total num_examples: 6181\n",
      "All done. Record file is:\n",
      "datasets/processed_merge_val.record\n"
     ]
    }
   ],
   "source": [
    "name='600+500_processed'\n",
    "create_tfrecord(path= 'datasets/'+name+\"/\", types=['png','jpg','jepg','png', 'bmp'], tf_record='datasets/'+name, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa326f-43af-4dbb-890a-531af22b71a1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdd0f44-d827-43b0-b3fe-321d94100ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_model, name = \"hrnetv2\", train_files = \"datasets/train.record\", test_files = None, val_files = None, input_shape = (128, 128, 3),\n",
    "          num_ids = 600, num_examples = 12000, training_dir = os.getcwd(),\n",
    "          val_num_ids = 600, val_num_examples = 12000, val_batch_size = 8, val_freq=None,\n",
    "          frequency = 1000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = 8, export_only = False,\n",
    "          override = False, epochs = 50, restore_weights_only=False):\n",
    "\n",
    "    '''\n",
    "    # Deep neural network training is complicated. The first thing is making\n",
    "    # sure you have everything ready for training, like datasets, checkpoints,\n",
    "    # logs, etc. Modify these paths to suit your needs.\n",
    "\n",
    "    name:str = # What is the model's name?\n",
    "    \n",
    "    train_files:str = # Where are the training files?\n",
    "\n",
    "    test_files:str = # Where are the testing files?\n",
    "\n",
    "    val_files:str = # Where are the validation files? Set `None` if no files available.\n",
    "\n",
    "    input_shape:tuple(int) = # What is the shape of the input image?\n",
    "\n",
    "    embedding_size:int = # What is the size of the embeddings that represent the faces?\n",
    "\n",
    "    num_ids:int = # How many identities do you have in the training dataset?\n",
    "\n",
    "    num_examples:int = # How many examples do you have in the training dataset?\n",
    "\n",
    "    # That should be sufficient for training. However if you want more\n",
    "    # customization, please keep going.\n",
    "\n",
    "    training_dir:str = # Where is the training direcotory for checkpoints and logs?\n",
    "\n",
    "    regularizer = # Any weight regularization?\n",
    "\n",
    "    frequency:int = # How often do you want to log and save the model, in steps?\n",
    "\n",
    "    # All sets. Now it's time to build the model. There are two steps in ArcFace\n",
    "    # training: 1, training with softmax loss; 2, training with arcloss. This\n",
    "    # means not only different loss functions but also fragmented models.\n",
    "\n",
    "    base_model:model = # First model is base model which outputs the face embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Where is the exported model going to be saved?\n",
    "    export_dir = os.path.join(training_dir, 'exported', name)\n",
    "    \n",
    "#     data_augmentation = tf.keras.Sequential([\n",
    "#        tf.keras.layers.RandomRotation(factor=0.05),\n",
    "#        tf.keras.layers.RandomContrast(factor=0.2),\n",
    "#        tf.keras.layers.RandomZoom(height_factor=0.1)\n",
    "#      ])\n",
    "    \n",
    "    # Then build the second model for training.\n",
    "    if softmax:\n",
    "        print(\"Building training model with softmax loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  # data_augmentation,\n",
    "                                  base_model,\n",
    "                                  keras.layers.Dense(num_ids,\n",
    "                                                     kernel_regularizer=regularizer),\n",
    "                                  keras.layers.Softmax()],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = keras.losses.CategoricalCrossentropy()\n",
    "    else:\n",
    "        print(\"Building training model with ARC loss...\")\n",
    "        model = keras.Sequential([keras.Input(input_shape),\n",
    "                                  # data_augmentation,\n",
    "                                  base_model,\n",
    "                                  L2Normalization(),\n",
    "                                  ArcLayer(num_ids, regularizer)],\n",
    "                                 name=\"training_model\")\n",
    "        loss_fun = ArcLoss()\n",
    "\n",
    "    # Summary the model to find any thing suspicious at early stage.\n",
    "    model.summary()\n",
    "\n",
    "    # Construct an optimizer. This optimizer is different from the official\n",
    "    # implementation which use SGD with momentum.\n",
    "    optimizer = keras.optimizers.Adam(adam_alpha, amsgrad=True, epsilon=adam_epsilon)\n",
    "\n",
    "    # Construct training datasets.\n",
    "    dataset_train = build_dataset(train_files,\n",
    "                                  batch_size=batch_size,\n",
    "                                  one_hot_depth=num_ids,\n",
    "                                  training=True,\n",
    "                                  buffer_size=4096)\n",
    "\n",
    "    # Construct dataset for validation. The loss value from this dataset can be\n",
    "    # used to decide which checkpoint should be preserved.\n",
    "    if val_files:\n",
    "        dataset_val = build_dataset(val_files,\n",
    "                                    batch_size=val_batch_size,\n",
    "                                    one_hot_depth=val_num_ids,\n",
    "                                    training=False,\n",
    "                                    buffer_size=4096)\n",
    "\n",
    "    # The training adventure is long and full of traps. A training supervisor\n",
    "    # can help us to ease the pain.\n",
    "    supervisor = TrainingSupervisor(model = model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    loss=loss_fun,\n",
    "                                    dataset=dataset_train,\n",
    "                                    training_dir=training_dir,\n",
    "                                    save_freq=frequency,\n",
    "                                    monitor=\"categorical_accuracy\",\n",
    "                                    mode='max',\n",
    "                                    name = name,\n",
    "                                    val_freq = val_freq,\n",
    "                                    val_dataset = dataset_val if val_files else None,\n",
    "                                   )\n",
    "\n",
    "    # If training accomplished, save the base model for inference.\n",
    "    if export_only:\n",
    "        print(\"The best model will be exported.\")\n",
    "        supervisor.restore(restore_weights_only, True)\n",
    "        supervisor.export(base_model, export_dir)\n",
    "        quit()\n",
    "\n",
    "    # Restore the latest model if checkpoints are available.\n",
    "    supervisor.restore(restore_weights_only)\n",
    "\n",
    "    # Sometimes the training process might go wrong and we would like to resume\n",
    "    # training from manually selected checkpoint. In this case some training\n",
    "    # objects should be overridden before training started.\n",
    "    if override:\n",
    "        supervisor.override(0, 1)\n",
    "        print(\"Training process overridden by user.\")\n",
    "\n",
    "    # Now it is safe to start training.\n",
    "    supervisor.train(epochs, num_examples // batch_size, val_num_examples // val_batch_size if val_files else None)\n",
    "\n",
    "    # Export the model after training.\n",
    "    supervisor.export(base_model, export_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e159f-666a-4fd2-9bfc-06021f71f8f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model with softmax loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_model (Functional) (None, 512)               43685888  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1142)              585846    \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 1142)              0         \n",
      "=================================================================\n",
      "Total params: 44,271,734\n",
      "Trainable params: 44,168,950\n",
      "Non-trainable params: 102,784\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f3ba25b2160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f3ba25b2160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f3ba25b2280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function build_dataset.<locals>._parse_function at 0x7f3ba25b2280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 20:57:57.982306: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-27 20:57:57.983128: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3191995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Checkpoint not found. Model will be initialized                 from scratch.\n",
      "Restoring..\n",
      "Only the model weights will be restored.\n",
      "Checkpoint restored: None\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;28;212;28m-------------------------------------------------------------------------------------------------\u001b[0m| 0/1545 [00:00<?, ?it/s]\u001b[0m2022-07-27 20:58:05.629363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-27 20:58:05.949889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-27 20:58:07.089458: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-07-27 20:58:07.132675: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3ba2bdd310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TrainingSupervisor._update_metrics of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f3ba2bdd310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:45<00:00,  6.24it/s, loss=2.79, accuracy=0.032]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0321, mean loss: 2.94\n",
      "Monitor value improved from 0.0000 to 0.0321.\n",
      "Best model found and saved: outputs/model_scout/processed_merge_soft/ckpt-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:55<00:00,  5.24it/s, loss=2.79, accuracy=0.032]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1545, to file: outputs/checkpoints/processed_merge_soft/ckpt-2\n",
      "\n",
      "Epoch 2/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:09<00:00,  6.22it/s, loss=26.18, accuracy=0.034]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0339, mean loss: 6.03\n",
      "Monitor value improved from 0.0321 to 0.0339.\n",
      "Best model found and saved: outputs/model_scout/processed_merge_soft/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:18<00:00,  5.98it/s, loss=26.18, accuracy=0.034]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 3090, to file: outputs/checkpoints/processed_merge_soft/ckpt-4\n",
      "\n",
      "Epoch 3/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:10<00:00,  6.08it/s, loss=54.63, accuracy=0.031]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0312, mean loss: 16.15\n",
      "Monitor value not improved: 0.0339, latest: 0.0312.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:13<00:00,  6.09it/s, loss=54.63, accuracy=0.031]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 4635, to file: outputs/checkpoints/processed_merge_soft/ckpt-5\n",
      "\n",
      "Epoch 4/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:10<00:00,  6.23it/s, loss=84.56, accuracy=0.029]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0293, mean loss: 28.77\n",
      "Monitor value not improved: 0.0339, latest: 0.0293.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:14<00:00,  6.08it/s, loss=84.56, accuracy=0.029]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 6180, to file: outputs/checkpoints/processed_merge_soft/ckpt-6\n",
      "\n",
      "Epoch 5/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:10<00:00,  6.18it/s, loss=111.64, accuracy=0.028]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0279, mean loss: 42.32\n",
      "Monitor value not improved: 0.0339, latest: 0.0279.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:14<00:00,  6.08it/s, loss=111.64, accuracy=0.028]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 7725, to file: outputs/checkpoints/processed_merge_soft/ckpt-7\n",
      "\n",
      "Epoch 6/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:11<00:00,  6.17it/s, loss=137.71, accuracy=0.027]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0271, mean loss: 56.01\n",
      "Monitor value not improved: 0.0339, latest: 0.0271.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 1545/1545 [04:15<00:00,  6.06it/s, loss=137.71, accuracy=0.027]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 9270, to file: outputs/checkpoints/processed_merge_soft/ckpt-8\n",
      "\n",
      "Epoch 7/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 730/1545 [01:58<02:12,  6.16it/s, loss=141.19, accuracy=0.028]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0280, mean loss: 62.91\n",
      "Monitor value not improved: 0.0339, latest: 0.0280.\n",
      "Checkpoint saved at global step: 10000, to file: outputs/checkpoints/processed_merge_soft/ckpt-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 1501/1545 [04:07<00:07,  6.18it/s, loss=162.64, accuracy=0.026]\u001b[0m"
     ]
    }
   ],
   "source": [
    "name='processed_merge'\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "embedding_size = 512\n",
    "num_ids = 1142\n",
    "num_examples = 24721\n",
    "val_num_ids = 1142\n",
    "val_num_examples= 6181\n",
    "val_batch_size = 16\n",
    "batch_size = 16\n",
    "epochs = 12\n",
    "\n",
    "train_files = 'datasets/'+name+'_train.record'\n",
    "val_files = 'datasets/'+name+'_val.record'\n",
    "checkpoint_dir = 'outputs/exported/'+ name +'_soft/'\n",
    "\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "base_model = resnet101(input_shape=input_shape, output_size=embedding_size,\n",
    "                           trainable=True,\n",
    "                           training=True,\n",
    "                           kernel_regularizer=regularizer,\n",
    "                           name=\"embedding_model\")\n",
    "\n",
    "train(base_model, name = name+\"_soft\", train_files = train_files, test_files = None, val_files = val_files, input_shape = input_shape,\n",
    "          num_ids = num_ids, num_examples = num_examples, training_dir = 'outputs/', val_num_ids = val_num_ids, val_num_examples = val_num_examples,\n",
    "          val_batch_size = val_batch_size, val_freq = 5, \n",
    "          frequency = 10000, softmax = True, adam_alpha=0.001, adam_epsilon=0.001, batch_size = batch_size, export_only = False,\n",
    "          override = False, epochs = epochs, restore_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dddbb-f81b-4dec-bab4-ac2ee2993af9",
   "metadata": {},
   "source": [
    "# Load softmax model and train with softmax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493cf1ea-809f-4ffe-a107-224f654b20ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Building training model with ARC loss...\n",
      "Model: \"training_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_1 (Sequential)   (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " embedding_model (Functional  (None, 512)              43685888  \n",
      " )                                                               \n",
      "                                                                 \n",
      " l2_normalization (L2Normali  (None, 512)              0         \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " arc_layer (ArcLayer)        (None, 1142)              584704    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,270,592\n",
      "Trainable params: 44,167,808\n",
      "Non-trainable params: 102,784\n",
      "_________________________________________________________________\n",
      "WARNING: Checkpoint not found. Model will be initialized                 from scratch.\n",
      "Restoring..\n",
      "Only the model weights will be restored.\n",
      "Checkpoint restored: None\n",
      "Resume training from global step: 0, epoch: 1\n",
      "Current step is: 0\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  4.45it/s, loss=2.02, accuracy=0.032]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0321, mean loss: 16.05\n",
      "Monitor value improved from 0.0000 to 0.0321.\n",
      "Best model found and saved: outputs/model_scout/processed_merge_arc/ckpt-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:35<00:00,  3.82it/s, loss=2.02, accuracy=0.032]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 135, to file: outputs/checkpoints/processed_merge_arc/ckpt-2\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:30<00:00,  4.42it/s, loss=1.62, accuracy=0.034]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0340, mean loss: 14.89\n",
      "Monitor value improved from 0.0321 to 0.0340.\n",
      "Best model found and saved: outputs/model_scout/processed_merge_arc/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.26it/s, loss=1.62, accuracy=0.034]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 270, to file: outputs/checkpoints/processed_merge_arc/ckpt-4\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:30<00:00,  4.38it/s, loss=1.27, accuracy=0.034]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0337, mean loss: 13.04\n",
      "Monitor value not improved: 0.0340, latest: 0.0337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.31it/s, loss=1.27, accuracy=0.034]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 405, to file: outputs/checkpoints/processed_merge_arc/ckpt-5\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:30<00:00,  4.38it/s, loss=1.82, accuracy=0.033]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0330, mean loss: 12.03\n",
      "Monitor value not improved: 0.0340, latest: 0.0330.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.28it/s, loss=1.82, accuracy=0.033]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 540, to file: outputs/checkpoints/processed_merge_arc/ckpt-6\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:30<00:00,  4.36it/s, loss=2.21, accuracy=0.031]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Epoch 5/20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|\u001b[38;2;28;212;28m--------------------------------------------------------------------------------------------\u001b[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "  2%|\u001b[38;2;28;212;28m>-----------------------------------------------------------------------------------\u001b[0m| 1/56 [00:00<00:24,  2.24it/s]\u001b[0m\u001b[A\n",
      "  2%|\u001b[38;2;28;212;28m>-------------------------------------------------------\u001b[0m| 1/56 [00:00<00:24,  2.24it/s, loss=23.35, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:24,  2.24it/s, loss=23.35, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  5%|\u001b[38;2;28;212;28m>>>-----------------------------------------------------\u001b[0m| 3/56 [00:00<00:08,  5.96it/s, loss=23.35, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  5%|\u001b[38;2;28;212;28m>>>-----------------------------------------------------\u001b[0m| 3/56 [00:00<00:08,  5.96it/s, loss=23.34, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:08,  5.96it/s, loss=23.33, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  9%|\u001b[38;2;28;212;28m>>>>>---------------------------------------------------\u001b[0m| 5/56 [00:00<00:05,  8.56it/s, loss=23.33, accuracy=0.031]\u001b[0m\u001b[A\n",
      "  9%|\u001b[38;2;28;212;28m>>>>>---------------------------------------------------\u001b[0m| 5/56 [00:00<00:05,  8.56it/s, loss=23.34, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:05,  8.56it/s, loss=23.34, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 12%|\u001b[38;2;28;212;28m>>>>>>>-------------------------------------------------\u001b[0m| 7/56 [00:00<00:04, 10.26it/s, loss=23.34, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 12%|\u001b[38;2;28;212;28m>>>>>>>-------------------------------------------------\u001b[0m| 7/56 [00:00<00:04, 10.26it/s, loss=23.33, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:04, 10.26it/s, loss=23.32, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 16%|\u001b[38;2;28;212;28m>>>>>>>>>-----------------------------------------------\u001b[0m| 9/56 [00:01<00:04, 10.69it/s, loss=23.32, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 16%|\u001b[38;2;28;212;28m>>>>>>>>>-----------------------------------------------\u001b[0m| 9/56 [00:01<00:04, 10.69it/s, loss=23.32, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:01<00:04, 10.69it/s, loss=23.31, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 20%|\u001b[38;2;28;212;28m>>>>>>>>>>---------------------------------------------\u001b[0m| 11/56 [00:01<00:03, 11.66it/s, loss=23.31, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 20%|\u001b[38;2;28;212;28m>>>>>>>>>>---------------------------------------------\u001b[0m| 11/56 [00:01<00:03, 11.66it/s, loss=23.30, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:01<00:03, 11.66it/s, loss=23.27, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 23%|\u001b[38;2;28;212;28m>>>>>>>>>>>>-------------------------------------------\u001b[0m| 13/56 [00:01<00:03, 12.47it/s, loss=23.27, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 23%|\u001b[38;2;28;212;28m>>>>>>>>>>>>-------------------------------------------\u001b[0m| 13/56 [00:01<00:03, 12.47it/s, loss=23.28, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:03, 12.47it/s, loss=23.29, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 27%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>-----------------------------------------\u001b[0m| 15/56 [00:01<00:03, 13.12it/s, loss=23.29, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 27%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>-----------------------------------------\u001b[0m| 15/56 [00:01<00:03, 13.12it/s, loss=23.25, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:03, 13.12it/s, loss=23.26, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 30%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>---------------------------------------\u001b[0m| 17/56 [00:01<00:02, 13.29it/s, loss=23.26, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 30%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>---------------------------------------\u001b[0m| 17/56 [00:01<00:02, 13.29it/s, loss=23.23, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 13.29it/s, loss=23.23, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 34%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>-------------------------------------\u001b[0m| 19/56 [00:01<00:02, 13.58it/s, loss=23.23, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 34%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>-------------------------------------\u001b[0m| 19/56 [00:01<00:02, 13.58it/s, loss=23.24, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 13.58it/s, loss=23.16, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 38%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 21/56 [00:01<00:02, 13.85it/s, loss=23.16, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 38%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 21/56 [00:01<00:02, 13.85it/s, loss=23.25, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 13.85it/s, loss=23.13, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 23/56 [00:02<00:02, 13.90it/s, loss=23.13, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 23/56 [00:02<00:02, 13.90it/s, loss=23.16, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:02<00:02, 13.90it/s, loss=23.16, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 45%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 25/56 [00:02<00:02, 14.18it/s, loss=23.16, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 45%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 25/56 [00:02<00:02, 14.18it/s, loss=23.05, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:02<00:02, 14.18it/s, loss=22.93, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 48%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------------\u001b[0m| 27/56 [00:02<00:02, 13.99it/s, loss=22.93, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 48%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------------\u001b[0m| 27/56 [00:02<00:02, 13.99it/s, loss=22.93, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:02, 13.99it/s, loss=22.83, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 52%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------------\u001b[0m| 29/56 [00:02<00:01, 14.18it/s, loss=22.83, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 52%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------------\u001b[0m| 29/56 [00:02<00:01, 14.18it/s, loss=23.01, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:01, 14.18it/s, loss=22.35, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 55%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------\u001b[0m| 31/56 [00:02<00:01, 14.48it/s, loss=22.35, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 55%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------\u001b[0m| 31/56 [00:02<00:01, 14.48it/s, loss=22.53, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 14.48it/s, loss=22.74, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 59%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------\u001b[0m| 33/56 [00:02<00:01, 14.30it/s, loss=22.74, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 59%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------\u001b[0m| 33/56 [00:02<00:01, 14.30it/s, loss=22.67, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 14.30it/s, loss=22.02, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 62%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------\u001b[0m| 35/56 [00:02<00:01, 14.15it/s, loss=22.02, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 62%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------\u001b[0m| 35/56 [00:02<00:01, 14.15it/s, loss=21.97, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 14.15it/s, loss=22.63, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 66%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------\u001b[0m| 37/56 [00:03<00:01, 14.27it/s, loss=22.63, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 66%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------\u001b[0m| 37/56 [00:03<00:01, 14.27it/s, loss=22.56, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:03<00:01, 14.27it/s, loss=22.29, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 70%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 39/56 [00:03<00:01, 14.61it/s, loss=22.29, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 70%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 39/56 [00:03<00:01, 14.61it/s, loss=22.54, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:03<00:01, 14.61it/s, loss=22.17, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 73%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------\u001b[0m| 41/56 [00:03<00:01, 14.45it/s, loss=22.17, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 73%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------\u001b[0m| 41/56 [00:03<00:01, 14.45it/s, loss=22.17, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:03<00:00, 14.45it/s, loss=22.17, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 77%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------\u001b[0m| 43/56 [00:03<00:00, 14.33it/s, loss=22.17, accuracy=0.030]\u001b[0m\u001b[A\n",
      " 77%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------\u001b[0m| 43/56 [00:03<00:00, 14.33it/s, loss=20.96, accuracy=0.031]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 14.33it/s, loss=20.56, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 80%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------\u001b[0m| 45/56 [00:03<00:00, 14.45it/s, loss=20.56, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 80%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------\u001b[0m| 45/56 [00:03<00:00, 14.45it/s, loss=20.85, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 14.45it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 84%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------\u001b[0m| 47/56 [00:03<00:00, 14.40it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 84%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------\u001b[0m| 47/56 [00:03<00:00, 14.40it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 14.40it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 88%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------\u001b[0m| 49/56 [00:03<00:00, 14.37it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 88%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------\u001b[0m| 49/56 [00:03<00:00, 14.37it/s, loss=20.92, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 14.37it/s, loss=12.52, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 91%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----\u001b[0m| 51/56 [00:03<00:00, 14.34it/s, loss=12.52, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 91%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----\u001b[0m| 51/56 [00:03<00:00, 14.34it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:04<00:00, 14.34it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 95%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---\u001b[0m| 53/56 [00:04<00:00,  9.60it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 95%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---\u001b[0m| 53/56 [00:04<00:00,  9.60it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:04<00:00,  9.60it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 98%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 55/56 [00:04<00:00, 10.05it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      " 98%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 55/56 [00:04<00:00, 10.05it/s, loss=0.83, accuracy=0.032]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:04<00:00, 12.18it/s, loss=0.83, accuracy=0.031]\u001b[0m\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0315, mean loss: 12.14\n",
      "Monitor value not improved: 0.0340, latest: 0.0315.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:36<00:00,  3.73it/s, loss=2.21, accuracy=0.031]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 675, to file: outputs/checkpoints/processed_merge_arc/ckpt-7\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:30<00:00,  4.40it/s, loss=1.32, accuracy=0.030]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0301, mean loss: 11.68\n",
      "Monitor value not improved: 0.0340, latest: 0.0301.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.26it/s, loss=1.32, accuracy=0.030]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 810, to file: outputs/checkpoints/processed_merge_arc/ckpt-8\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:36<00:00,  3.47it/s, loss=1.01, accuracy=0.029]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0291, mean loss: 11.34\n",
      "Monitor value not improved: 0.0340, latest: 0.0291.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:37<00:00,  3.62it/s, loss=1.01, accuracy=0.029]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 945, to file: outputs/checkpoints/processed_merge_arc/ckpt-9\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 55/135 [00:13<00:18,  4.22it/s, loss=7.44, accuracy=0.029]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0293, mean loss: 11.71\n",
      "Monitor value not improved: 0.0340, latest: 0.0293.\n",
      "Checkpoint saved at global step: 1000, to file: outputs/checkpoints/processed_merge_arc/ckpt-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.28it/s, loss=1.29, accuracy=0.028]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0282, mean loss: 11.07\n",
      "Monitor value not improved: 0.0340, latest: 0.0282.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  4.05it/s, loss=1.29, accuracy=0.028]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1080, to file: outputs/checkpoints/processed_merge_arc/ckpt-11\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.06it/s, loss=1.10, accuracy=0.028]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0276, mean loss: 10.85\n",
      "Monitor value not improved: 0.0340, latest: 0.0276.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.10it/s, loss=1.10, accuracy=0.028]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1215, to file: outputs/checkpoints/processed_merge_arc/ckpt-12\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.06it/s, loss=1.19, accuracy=0.027]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Epoch 10/20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|\u001b[38;2;28;212;28m--------------------------------------------------------------------------------------------\u001b[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "  2%|\u001b[38;2;28;212;28m>-------------------------------------------------------\u001b[0m| 1/56 [00:00<00:04, 11.85it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.51it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.51it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  5%|\u001b[38;2;28;212;28m>>>-----------------------------------------------------\u001b[0m| 3/56 [00:00<00:04, 12.51it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:03, 13.39it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:03, 13.39it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      "  9%|\u001b[38;2;28;212;28m>>>>>---------------------------------------------------\u001b[0m| 5/56 [00:00<00:03, 13.39it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 12%|\u001b[38;2;28;212;28m>>>>>>>-------------------------------------------------\u001b[0m| 7/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.53it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.53it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 16%|\u001b[38;2;28;212;28m>>>>>>>>>-----------------------------------------------\u001b[0m| 9/56 [00:00<00:03, 13.53it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.93it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.93it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 20%|\u001b[38;2;28;212;28m>>>>>>>>>>---------------------------------------------\u001b[0m| 11/56 [00:00<00:03, 13.93it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 23%|\u001b[38;2;28;212;28m>>>>>>>>>>>>-------------------------------------------\u001b[0m| 13/56 [00:00<00:03, 13.89it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:02, 14.08it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:02, 14.08it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 27%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>-----------------------------------------\u001b[0m| 15/56 [00:01<00:02, 14.08it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 13.46it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 13.46it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 30%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>---------------------------------------\u001b[0m| 17/56 [00:01<00:02, 13.46it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 12.77it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 12.77it/s, loss=22.90, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 34%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>-------------------------------------\u001b[0m| 19/56 [00:01<00:02, 12.77it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 13.13it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 13.13it/s, loss=22.91, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 38%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 21/56 [00:01<00:02, 13.13it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 12.22it/s, loss=22.92, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 12.22it/s, loss=22.85, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 23/56 [00:01<00:02, 12.22it/s, loss=22.84, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:02<00:03,  8.61it/s, loss=22.84, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:02<00:03,  8.61it/s, loss=22.87, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 45%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 25/56 [00:02<00:03,  8.61it/s, loss=22.84, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:02<00:03,  9.77it/s, loss=22.84, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:02<00:03,  9.77it/s, loss=22.76, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 48%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------------\u001b[0m| 27/56 [00:02<00:02,  9.77it/s, loss=22.74, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:02, 10.78it/s, loss=22.74, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:02, 10.78it/s, loss=22.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 52%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------------\u001b[0m| 29/56 [00:02<00:02, 10.78it/s, loss=22.77, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:02, 11.58it/s, loss=22.77, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:02, 11.58it/s, loss=22.08, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 55%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------\u001b[0m| 31/56 [00:02<00:02, 11.58it/s, loss=22.31, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 12.14it/s, loss=22.31, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 12.14it/s, loss=22.49, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 59%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------\u001b[0m| 33/56 [00:02<00:01, 12.14it/s, loss=22.50, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 12.69it/s, loss=22.50, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 12.69it/s, loss=21.79, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 62%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------\u001b[0m| 35/56 [00:02<00:01, 12.69it/s, loss=21.74, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 13.30it/s, loss=21.74, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 13.30it/s, loss=22.49, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 66%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------\u001b[0m| 37/56 [00:02<00:01, 13.30it/s, loss=22.31, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:03<00:01, 13.56it/s, loss=22.31, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:03<00:01, 13.56it/s, loss=22.03, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 70%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 39/56 [00:03<00:01, 13.56it/s, loss=22.37, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:03<00:01, 13.53it/s, loss=22.37, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:03<00:01, 13.53it/s, loss=22.12, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 73%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------\u001b[0m| 41/56 [00:03<00:01, 13.53it/s, loss=22.12, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:03<00:01, 13.66it/s, loss=22.12, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:03<00:01, 13.66it/s, loss=22.12, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 77%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------\u001b[0m| 43/56 [00:03<00:00, 13.66it/s, loss=20.73, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 13.84it/s, loss=20.73, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 13.84it/s, loss=20.27, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 80%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------\u001b[0m| 45/56 [00:03<00:00, 13.84it/s, loss=20.54, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 14.15it/s, loss=20.54, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 14.15it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 84%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------\u001b[0m| 47/56 [00:03<00:00, 14.15it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 14.31it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 14.31it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 88%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------\u001b[0m| 49/56 [00:03<00:00, 14.31it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 14.35it/s, loss=20.60, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 14.35it/s, loss=12.18, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 91%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----\u001b[0m| 51/56 [00:03<00:00, 14.35it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:04<00:00, 13.69it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:04<00:00, 13.69it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 95%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---\u001b[0m| 53/56 [00:04<00:00, 13.69it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:04<00:00, 13.89it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:04<00:00, 13.89it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      " 98%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 55/56 [00:04<00:00, 13.89it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:04<00:00, 14.23it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:04<00:00, 12.89it/s, loss=0.47, accuracy=0.027]\u001b[0m\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0271, mean loss: 11.02\n",
      "Monitor value not improved: 0.0340, latest: 0.0271.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:37<00:00,  3.59it/s, loss=1.19, accuracy=0.027]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1350, to file: outputs/checkpoints/processed_merge_arc/ckpt-13\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  3.89it/s, loss=0.78, accuracy=0.027]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0266, mean loss: 10.84\n",
      "Monitor value not improved: 0.0340, latest: 0.0266.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:34<00:00,  3.94it/s, loss=0.78, accuracy=0.027]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1485, to file: outputs/checkpoints/processed_merge_arc/ckpt-14\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.19it/s, loss=0.83, accuracy=0.026]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0261, mean loss: 10.68\n",
      "Monitor value not improved: 0.0340, latest: 0.0261.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  4.08it/s, loss=0.83, accuracy=0.026]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1620, to file: outputs/checkpoints/processed_merge_arc/ckpt-15\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  3.91it/s, loss=0.81, accuracy=0.026]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0259, mean loss: 10.55\n",
      "Monitor value not improved: 0.0340, latest: 0.0259.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:34<00:00,  3.97it/s, loss=0.81, accuracy=0.026]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1755, to file: outputs/checkpoints/processed_merge_arc/ckpt-16\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.18it/s, loss=0.76, accuracy=0.026]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0256, mean loss: 10.43\n",
      "Monitor value not improved: 0.0340, latest: 0.0256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.09it/s, loss=0.76, accuracy=0.026]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 1890, to file: outputs/checkpoints/processed_merge_arc/ckpt-17\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 110/135 [00:26<00:07,  3.35it/s, loss=0.59, accuracy=0.026]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0257, mean loss: 10.43\n",
      "Monitor value not improved: 0.0340, latest: 0.0257.\n",
      "Checkpoint saved at global step: 2000, to file: outputs/checkpoints/processed_merge_arc/ckpt-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:34<00:00,  4.12it/s, loss=1.16, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Epoch 15/20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|\u001b[38;2;28;212;28m--------------------------------------------------------------------------------------------\u001b[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "  2%|\u001b[38;2;28;212;28m>-------------------------------------------------------\u001b[0m| 1/56 [00:00<00:05, 10.99it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.10it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.10it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  5%|\u001b[38;2;28;212;28m>>>-----------------------------------------------------\u001b[0m| 3/56 [00:00<00:04, 12.10it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:03, 13.40it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:03, 13.40it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  9%|\u001b[38;2;28;212;28m>>>>>---------------------------------------------------\u001b[0m| 5/56 [00:00<00:03, 13.40it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.44it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.44it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 12%|\u001b[38;2;28;212;28m>>>>>>>-------------------------------------------------\u001b[0m| 7/56 [00:00<00:03, 13.44it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.78it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.78it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 16%|\u001b[38;2;28;212;28m>>>>>>>>>-----------------------------------------------\u001b[0m| 9/56 [00:00<00:03, 13.78it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.94it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.94it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 20%|\u001b[38;2;28;212;28m>>>>>>>>>>---------------------------------------------\u001b[0m| 11/56 [00:00<00:03, 13.94it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.98it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.98it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 23%|\u001b[38;2;28;212;28m>>>>>>>>>>>>-------------------------------------------\u001b[0m| 13/56 [00:00<00:03, 13.98it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:02, 14.07it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:02, 14.07it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 27%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>-----------------------------------------\u001b[0m| 15/56 [00:01<00:02, 14.07it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 14.03it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 14.03it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 30%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>---------------------------------------\u001b[0m| 17/56 [00:01<00:02, 14.03it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 14.04it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 14.04it/s, loss=22.70, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 34%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>-------------------------------------\u001b[0m| 19/56 [00:01<00:02, 14.04it/s, loss=22.70, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 14.19it/s, loss=22.70, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 14.19it/s, loss=22.70, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 38%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 21/56 [00:01<00:02, 14.19it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 14.23it/s, loss=22.71, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 14.23it/s, loss=22.66, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 23/56 [00:01<00:02, 14.23it/s, loss=22.66, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:01<00:02, 14.08it/s, loss=22.66, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:01<00:02, 14.08it/s, loss=22.67, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 45%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 25/56 [00:01<00:02, 14.08it/s, loss=22.66, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:01<00:02, 13.95it/s, loss=22.66, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:01<00:02, 13.95it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 48%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------------\u001b[0m| 27/56 [00:01<00:02, 13.95it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:01, 14.05it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:01, 14.05it/s, loss=22.46, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 52%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------------\u001b[0m| 29/56 [00:02<00:01, 14.05it/s, loss=22.57, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:01, 14.22it/s, loss=22.57, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:01, 14.22it/s, loss=21.90, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 55%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------\u001b[0m| 31/56 [00:02<00:01, 14.22it/s, loss=22.14, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 14.44it/s, loss=22.14, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 14.44it/s, loss=22.35, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 59%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------\u001b[0m| 33/56 [00:02<00:01, 14.44it/s, loss=22.40, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 14.52it/s, loss=22.40, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 14.52it/s, loss=21.63, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 62%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------\u001b[0m| 35/56 [00:02<00:01, 14.52it/s, loss=21.57, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 14.44it/s, loss=21.57, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 14.44it/s, loss=22.31, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 66%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------\u001b[0m| 37/56 [00:02<00:01, 14.44it/s, loss=22.16, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:02<00:01, 14.35it/s, loss=22.16, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:02<00:01, 14.35it/s, loss=21.83, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 70%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 39/56 [00:02<00:01, 14.35it/s, loss=22.20, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:02<00:01, 14.42it/s, loss=22.20, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:02<00:01, 14.42it/s, loss=21.91, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 73%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------\u001b[0m| 41/56 [00:02<00:01, 14.42it/s, loss=21.91, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:02<00:00, 14.20it/s, loss=21.91, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:02<00:00, 14.20it/s, loss=21.91, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 77%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------\u001b[0m| 43/56 [00:03<00:00, 14.20it/s, loss=20.44, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 14.11it/s, loss=20.44, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 14.11it/s, loss=19.97, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 80%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------\u001b[0m| 45/56 [00:03<00:00, 14.11it/s, loss=20.37, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 14.18it/s, loss=20.37, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 14.18it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 84%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------\u001b[0m| 47/56 [00:03<00:00, 14.18it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 14.17it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 14.17it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 88%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------\u001b[0m| 49/56 [00:03<00:00, 14.17it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 14.03it/s, loss=20.46, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 14.03it/s, loss=12.01, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 91%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----\u001b[0m| 51/56 [00:03<00:00, 14.03it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:03<00:00, 13.98it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:03<00:00, 13.98it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 95%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---\u001b[0m| 53/56 [00:03<00:00, 13.98it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:03<00:00, 14.22it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:03<00:00, 14.22it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      " 98%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 55/56 [00:03<00:00, 14.22it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:03<00:00, 14.52it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:03<00:00, 14.09it/s, loss=0.26, accuracy=0.026]\u001b[0m\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0256, mean loss: 10.56\n",
      "Monitor value not improved: 0.0340, latest: 0.0256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:39<00:00,  3.45it/s, loss=1.16, accuracy=0.025]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2025, to file: outputs/checkpoints/processed_merge_arc/ckpt-19\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:35<00:00,  3.33it/s, loss=0.69, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0254, mean loss: 10.45\n",
      "Monitor value not improved: 0.0340, latest: 0.0254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:36<00:00,  3.74it/s, loss=0.69, accuracy=0.025]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2160, to file: outputs/checkpoints/processed_merge_arc/ckpt-20\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.10it/s, loss=0.21, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0252, mean loss: 10.35\n",
      "Monitor value not improved: 0.0340, latest: 0.0252.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:33<00:00,  4.03it/s, loss=0.21, accuracy=0.025]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2295, to file: outputs/checkpoints/processed_merge_arc/ckpt-21\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.28it/s, loss=1.01, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0250, mean loss: 10.27\n",
      "Monitor value not improved: 0.0340, latest: 0.0250.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:32<00:00,  4.17it/s, loss=1.01, accuracy=0.025]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2430, to file: outputs/checkpoints/processed_merge_arc/ckpt-22\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:38<00:00,  3.81it/s, loss=0.63, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0249, mean loss: 10.18\n",
      "Monitor value not improved: 0.0340, latest: 0.0249.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:39<00:00,  3.44it/s, loss=0.63, accuracy=0.025]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2565, to file: outputs/checkpoints/processed_merge_arc/ckpt-23\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:31<00:00,  4.28it/s, loss=0.94, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Epoch 20/20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|\u001b[38;2;28;212;28m--------------------------------------------------------------------------------------------\u001b[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "  2%|\u001b[38;2;28;212;28m>-------------------------------------------------------\u001b[0m| 1/56 [00:00<00:05, 10.73it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.17it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  4%|\u001b[38;2;28;212;28m>>------------------------------------------------------\u001b[0m| 2/56 [00:00<00:04, 12.17it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  5%|\u001b[38;2;28;212;28m>>>-----------------------------------------------------\u001b[0m| 3/56 [00:00<00:04, 12.17it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:04, 12.86it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  7%|\u001b[38;2;28;212;28m>>>>----------------------------------------------------\u001b[0m| 4/56 [00:00<00:04, 12.86it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      "  9%|\u001b[38;2;28;212;28m>>>>>---------------------------------------------------\u001b[0m| 5/56 [00:00<00:03, 12.86it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.36it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 11%|\u001b[38;2;28;212;28m>>>>>>--------------------------------------------------\u001b[0m| 6/56 [00:00<00:03, 13.36it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 12%|\u001b[38;2;28;212;28m>>>>>>>-------------------------------------------------\u001b[0m| 7/56 [00:00<00:03, 13.36it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.44it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 14%|\u001b[38;2;28;212;28m>>>>>>>>------------------------------------------------\u001b[0m| 8/56 [00:00<00:03, 13.44it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 16%|\u001b[38;2;28;212;28m>>>>>>>>>-----------------------------------------------\u001b[0m| 9/56 [00:00<00:03, 13.44it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.72it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 18%|\u001b[38;2;28;212;28m>>>>>>>>>----------------------------------------------\u001b[0m| 10/56 [00:00<00:03, 13.72it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 20%|\u001b[38;2;28;212;28m>>>>>>>>>>---------------------------------------------\u001b[0m| 11/56 [00:00<00:03, 13.72it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.90it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 21%|\u001b[38;2;28;212;28m>>>>>>>>>>>--------------------------------------------\u001b[0m| 12/56 [00:00<00:03, 13.90it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 23%|\u001b[38;2;28;212;28m>>>>>>>>>>>>-------------------------------------------\u001b[0m| 13/56 [00:00<00:03, 13.90it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:03, 13.87it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 25%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>------------------------------------------\u001b[0m| 14/56 [00:01<00:03, 13.87it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 27%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>-----------------------------------------\u001b[0m| 15/56 [00:01<00:02, 13.87it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 13.69it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 29%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>----------------------------------------\u001b[0m| 16/56 [00:01<00:02, 13.69it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 30%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>---------------------------------------\u001b[0m| 17/56 [00:01<00:02, 13.69it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 13.73it/s, loss=22.60, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 32%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>--------------------------------------\u001b[0m| 18/56 [00:01<00:02, 13.73it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 34%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>-------------------------------------\u001b[0m| 19/56 [00:01<00:02, 13.73it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 13.88it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 36%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>------------------------------------\u001b[0m| 20/56 [00:01<00:02, 13.88it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 38%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>-----------------------------------\u001b[0m| 21/56 [00:01<00:02, 13.88it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 14.13it/s, loss=22.59, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 39%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>----------------------------------\u001b[0m| 22/56 [00:01<00:02, 14.13it/s, loss=22.56, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 41%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>---------------------------------\u001b[0m| 23/56 [00:01<00:02, 14.13it/s, loss=22.56, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:01<00:02, 14.21it/s, loss=22.56, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 43%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>--------------------------------\u001b[0m| 24/56 [00:01<00:02, 14.21it/s, loss=22.57, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 45%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>-------------------------------\u001b[0m| 25/56 [00:01<00:02, 14.21it/s, loss=22.56, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:01<00:02, 14.17it/s, loss=22.56, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 46%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>------------------------------\u001b[0m| 26/56 [00:01<00:02, 14.17it/s, loss=22.53, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 48%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------------\u001b[0m| 27/56 [00:01<00:02, 14.17it/s, loss=22.51, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:01, 14.02it/s, loss=22.51, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 50%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------------\u001b[0m| 28/56 [00:02<00:01, 14.02it/s, loss=22.33, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 52%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------------\u001b[0m| 29/56 [00:02<00:01, 14.02it/s, loss=22.47, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:01, 14.21it/s, loss=22.47, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 54%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------------\u001b[0m| 30/56 [00:02<00:01, 14.21it/s, loss=21.75, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 55%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------------\u001b[0m| 31/56 [00:02<00:01, 14.21it/s, loss=22.07, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 14.11it/s, loss=22.07, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 57%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------------\u001b[0m| 32/56 [00:02<00:01, 14.11it/s, loss=22.28, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 59%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------------\u001b[0m| 33/56 [00:02<00:01, 14.11it/s, loss=22.35, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 14.20it/s, loss=22.35, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 61%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------------\u001b[0m| 34/56 [00:02<00:01, 14.20it/s, loss=21.48, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 62%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------------\u001b[0m| 35/56 [00:02<00:01, 14.20it/s, loss=21.44, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 14.06it/s, loss=21.44, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 64%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------------\u001b[0m| 36/56 [00:02<00:01, 14.06it/s, loss=22.26, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 66%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------------\u001b[0m| 37/56 [00:02<00:01, 14.06it/s, loss=22.07, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:02<00:01, 14.12it/s, loss=22.07, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 68%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------------\u001b[0m| 38/56 [00:02<00:01, 14.12it/s, loss=21.68, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 70%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------------\u001b[0m| 39/56 [00:02<00:01, 14.12it/s, loss=22.19, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:02<00:01, 14.07it/s, loss=22.19, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 71%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------------\u001b[0m| 40/56 [00:02<00:01, 14.07it/s, loss=21.77, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 73%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------------\u001b[0m| 41/56 [00:02<00:01, 14.07it/s, loss=21.77, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:03<00:00, 14.16it/s, loss=21.77, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 75%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------------\u001b[0m| 42/56 [00:03<00:00, 14.16it/s, loss=21.77, accuracy=0.024]\u001b[0m\u001b[A\n",
      " 77%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------------\u001b[0m| 43/56 [00:03<00:00, 14.16it/s, loss=20.34, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 14.12it/s, loss=20.34, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 79%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------------\u001b[0m| 44/56 [00:03<00:00, 14.12it/s, loss=19.88, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 80%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----------\u001b[0m| 45/56 [00:03<00:00, 14.12it/s, loss=20.30, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 13.65it/s, loss=20.30, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 82%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----------\u001b[0m| 46/56 [00:03<00:00, 13.65it/s, loss=20.38, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 84%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---------\u001b[0m| 47/56 [00:03<00:00, 13.65it/s, loss=20.39, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 13.86it/s, loss=20.39, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 86%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--------\u001b[0m| 48/56 [00:03<00:00, 13.86it/s, loss=20.39, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 88%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-------\u001b[0m| 49/56 [00:03<00:00, 13.86it/s, loss=20.39, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 13.96it/s, loss=20.39, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 89%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>------\u001b[0m| 50/56 [00:03<00:00, 13.96it/s, loss=11.92, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 91%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-----\u001b[0m| 51/56 [00:03<00:00, 13.96it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:03<00:00, 14.07it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 93%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>----\u001b[0m| 52/56 [00:03<00:00, 14.07it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 95%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>---\u001b[0m| 53/56 [00:03<00:00, 14.07it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:03<00:00, 14.24it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 96%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>--\u001b[0m| 54/56 [00:03<00:00, 14.24it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      " 98%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>-\u001b[0m| 55/56 [00:03<00:00, 14.24it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:04<00:00, 14.44it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n",
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 56/56 [00:04<00:00, 13.92it/s, loss=0.15, accuracy=0.025]\u001b[0m\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.0248, mean loss: 10.29\n",
      "Monitor value not improved: 0.0340, latest: 0.0248.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;28;212;28m>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[0m| 135/135 [00:36<00:00,  3.70it/s, loss=0.94, accuracy=0.025]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at global step: 2700, to file: outputs/checkpoints/processed_merge_arc/ckpt-24\n",
      "Training accomplished at epoch 20\n",
      "Saving model to outputs/exported/processed_merge_arc ...\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/exported/processed_merge_arc/assets\n",
      "Model saved at: outputs/exported/processed_merge_arc\n"
     ]
    }
   ],
   "source": [
    "name='processed_merge'\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "embedding_size = 512\n",
    "num_ids = 1142\n",
    "num_examples = 24721\n",
    "val_num_ids = 1142\n",
    "val_num_examples= 6181\n",
    "val_batch_size = 32\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "\n",
    "train_files = 'datasets/'+name+'_train.record'\n",
    "val_files = 'datasets/'+name+'_val.record'\n",
    "checkpoint_dir = 'outputs/exported/'+ name +'_soft/'\n",
    "\n",
    "regularizer = keras.regularizers.L2(5e-4)\n",
    "base_model = keras.models.load_model(checkpoint_dir)\n",
    "\n",
    "\n",
    "train(base_model, name = name+\"_arc\", train_files = train_files, test_files = None, val_files = val_files, input_shape = input_shape,\n",
    "          num_ids = num_ids, num_examples = num_examples, training_dir = 'outputs/', val_num_ids = val_num_ids, val_num_examples = val_num_examples,\n",
    "          val_batch_size = val_batch_size, val_freq = 5,\n",
    "          frequency = 10000, softmax = False, adam_alpha=0.001, adam_epsilon=0.001, batch_size = batch_size, export_only = False,\n",
    "          override = False, epochs = epochs, restore_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2381e95-27ed-4513-97c0-5b919c5db94c",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9627a76d-c6ed-4869-92d0-ead611b6ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(pkl, path = 'model.pkl'):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(pkl, f)\n",
    "    print(\"saved pkl file at:\",path)\n",
    "\n",
    "def load_pkl(path='model.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        pkl = pickle.load(f)\n",
    "    return pkl\n",
    "\n",
    "def findCosineDistance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculate cosine distance between two vector\n",
    "    \"\"\"\n",
    "    vec1 = vector1.flatten()\n",
    "    vec2 = vector2.flatten()\n",
    "\n",
    "    a = np.dot(vec1.T, vec2)\n",
    "    b = np.dot(vec1.T, vec1)\n",
    "    c = np.dot(vec2.T, vec2)\n",
    "    return 1 - (a/(np.sqrt(b)*np.sqrt(c)))\n",
    "\n",
    "def cosine_similarity(test_vec, source_vecs):\n",
    "    \"\"\"\n",
    "    Verify the similarity of one vector to group vectors of one class\n",
    "    \"\"\"\n",
    "    cos_dist = 0\n",
    "    for source_vec in source_vecs:\n",
    "        cos_dist += findCosineDistance(test_vec, source_vec)\n",
    "    return cos_dist/len(source_vecs)\n",
    "\n",
    "def make_embeddings(dataset_path='datasets/sorted_palmvein_roi/', output_path='outputs/', model_dir='outputs/exported/arcface', types='bmp'):\n",
    "    # Grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying palms...\")\n",
    "    # imagePaths =[]\n",
    "    # [imagePaths.extend(glob(dataset_path+\"*/*.\"+typ)) for typ in types]\n",
    "    \n",
    "    samples =[]\n",
    "    [samples.extend(glob(dataset_path+\"*/*.\"+typ)) for typ in types]\n",
    "    labels = [image_path.split('/')[-2] for image_path in samples]\n",
    "    sample_labels = [image_path.split('/')[-2] for image_path in samples]\n",
    "    samples_train, imagePaths = train_test_split(samples, test_size=0.2, random_state=42, stratify=sample_labels)\n",
    "    \n",
    "#     imagePaths.extend(glob(dataset_path+'/*/*.jpg'))\n",
    "\n",
    "    # Initialize model\n",
    "    embedding_model = keras.models.load_model(model_dir)\n",
    "       \n",
    "    # Initialize our lists of extracted facial embeddings and corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # Initialize the total number of faces processed\n",
    "    total = 0\n",
    "\n",
    "    # Loop over the imagePaths\n",
    "    for (i, imagePath) in tqdm(enumerate(imagePaths)):\n",
    "        # extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "#         print(imagePath)\n",
    "\n",
    "         # load the image\n",
    "        img = cv2.imread(imagePath).reshape(-1,128,128,3)\n",
    "        palms_embedding = embedding_model.predict(img)[0]\n",
    "        # add the name of the person + corresponding face\n",
    "        # embedding to their respective list\n",
    "        knownNames.append(name)\n",
    "        knownEmbeddings.append(palms_embedding)\n",
    "        total += 1\n",
    "        \n",
    "    print(total, \" palms embedded\")\n",
    "    print(f\"number of classes: {len(set(knownNames))}\")\n",
    "\n",
    "    # save to output\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    save_pkl(pkl=data, path=output_path+'db.pkl')\n",
    "    \n",
    "def make_model(embeddings_path='outputs/db.pkl', output_path='outputs/'):\n",
    "    # Load the face embeddings\n",
    "    data = load_pkl(embeddings_path)\n",
    "    num_classes = len(np.unique(data[\"names\"]))\n",
    "    y = np.array(data[\"names\"])\n",
    "    X = np.array(data[\"embeddings\"])\n",
    "    \n",
    "    \n",
    "    # Initialize Softmax training model arguments\n",
    "    input_shape = X.shape[1]\n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "    model =  MLPClassifier(hidden_layer_sizes=(input_shape, 640, 112, 640, num_classes), activation='tanh',max_iter=100, batch_size='auto', learning_rate='adaptive',\n",
    "                           validation_fraction=0.0, solver='adam', early_stopping=False ,verbose=0,random_state=1)\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx],)\n",
    "        print(model.score(X[valid_idx], y[valid_idx]), end='\\t')\n",
    "    \n",
    "    # save_pkl(pkl=model, path=output_path+'model.pkl')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c00b650-1ae4-4681-896c-118fa718fbe4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying palms...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6181it [07:17, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6181  palms embedded\n",
      "number of classes: 1142\n",
      "saved pkl file at: outputs/exported/processed_resnet_arc/db.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name=\"processed_resnet\"\n",
    "make_embeddings(dataset_path='datasets/processed_merge/', output_path='outputs/exported/'+name+'_arc/', model_dir='outputs/exported/'+name+'_arc/',types=['png','jpg','jepg','png', 'bmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca291fa-161b-4827-8d3a-67efb436ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30072756669361356\t0.32119741100323623\t0.30906148867313915\t"
     ]
    }
   ],
   "source": [
    "make_model(embeddings_path='outputs/exported/'+name+'_arc/db.pkl', output_path='outputs/exported/'+name+'_arc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f1fba4-736d-4895-8fb6-6ac165fc357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = keras.models.load_model('outputs/exported/'+name+'_arc/')\n",
    "model = load_pkl('outputs/exported/'+name+'_arc/model.pkl')\n",
    "samples = glob('datasets/processed_merge/*/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff4da8a9-c786-47d2-8974-208ea1d20d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583\t0.456\t0.488\t0.505\t0.483\t0.499\t0.549\t0.480\t0.48\t0.575\t0.815\t0.704\t0.680\t0.775\t0.830\t0.731\t0.787\t0.812\t0.8\t0.783\t\n",
      "milad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\t\n",
      "milad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tmilad_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\tDr.cheshomi_L\t\n"
     ]
    }
   ],
   "source": [
    "img_paths = samples[0:20]\n",
    "imgs = [cv2.imread(img).reshape(-1, 128, 128, 3) for img in img_paths]\n",
    "embedding = [embedding_model.predict(img)[0] for img in imgs]\n",
    "\n",
    "[print(np.format_float_positional((cosine_similarity(i, embedding[:10])), precision=3), end='\\t') for i in embedding]\n",
    "print()\n",
    "[print(i, end='\\t') for i in model.predict(embedding)]\n",
    "print()\n",
    "[print(img.split(\"/\")[-2], end='\\t') for img in img_paths] \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a56e6-f000-438b-8c52-90fa40b0b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573feb8-805c-49e4-a1d3-b6a51b75d126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recognize",
   "language": "python",
   "name": "recognize"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
